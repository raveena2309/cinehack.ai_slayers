{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ilh0OXNubYla",
        "outputId": "8fd3a2fc-37a7-4352-8391-fc05073c526c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.12/dist-packages (4.16.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (3.3.1)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "🔑 Please provide your X (Twitter) API Bearer Token:\n",
            "Enter Bearer Token: AAAAAAAAAAAAAAAAAAAAAFtX4gEAAAAArL%2BLuWsEbVS3mNkfHK3cyfrrfyA%3Dx5C7vZTmkkgYWerzReZ7qi39PFhfBd1DMNeHX9Arzg5Qkgwc8r\n",
            "🎬 Enter the movie name you want to monitor:\n",
            "Movie Name: Saiyaara\n",
            "How many official posters do you want to upload? 10\n",
            "📤 Please upload 10 official poster(s)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-81a0f5ca-96a1-4a79-8142-43523c5f3c5a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-81a0f5ca-96a1-4a79-8142-43523c5f3c5a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test 1.jpeg to test 1 (1).jpeg\n",
            "Saving test2.jpg to test2 (1).jpg\n",
            "Saving test3.jpg to test3 (1).jpg\n",
            "Saving test4.jpeg to test4 (1).jpeg\n",
            "Saving test5.jpeg to test5 (1).jpeg\n",
            "Saving test6.jpg to test6 (1).jpg\n",
            "Saving test7.webp to test7 (1).webp\n",
            "Saving test8.jpeg to test8 (1).jpeg\n",
            "Saving test9.jpg to test9 (1).jpg\n",
            "Saving test10.jpg to test10 (1).jpg\n",
            "✅ Official poster hashes stored in ledger.json\n",
            "🔎 Fetching candidate posters from X...\n",
            "📥 Downloaded 2 candidate posters.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "⚡ Classifying candidate posters...\n",
            "\n",
            "📊 Detection Report:\n",
            "\n",
            "              Tweet ID                                          Tweet URL  \\\n",
            "0  1973643569916498302  https://x.com/buzzzookashow/status/19736435699...   \n",
            "1  1973355872672948511  https://x.com/ocdtimes/status/1973355872672948511   \n",
            "\n",
            "        Username                  Posted At  \\\n",
            "0  buzzzookashow  2025-10-02 06:57:50+00:00   \n",
            "1       ocdtimes  2025-10-01 11:54:37+00:00   \n",
            "\n",
            "                                       Tweet Snippet  Similarity (%)  \\\n",
            "0  🎬 Poster coming soon! 👀\\nRandeep Hooda reporte...       65.410004   \n",
            "1  Poster coming soon bhi bol rahe hain.. reporte...       68.629997   \n",
            "\n",
            "  Classification  \n",
            "0  Irrelevant ⚠️  \n",
            "1  Irrelevant ⚠️  \n",
            "\n",
            "✅ Report saved as poster_detection_report.csv\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# Fake Poster Detection Pipeline with X API (90% Threshold)\n",
        "# ================================\n",
        "\n",
        "!pip install tweepy tensorflow pillow pandas\n",
        "\n",
        "import os, json, hashlib, requests, pandas as pd\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tweepy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from google.colab import files\n",
        "\n",
        "# ------------------------\n",
        "# Step 0: Setup\n",
        "# ------------------------\n",
        "print(\"🔑 Please provide your X (Twitter) API Bearer Token:\")\n",
        "BEARER_TOKEN = input(\"Enter Bearer Token: \").strip()\n",
        "\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN, wait_on_rate_limit=True)\n",
        "\n",
        "\n",
        "print(\"🎬 Enter the movie name you want to monitor:\")\n",
        "MOVIE_NAME = input(\"Movie Name: \")\n",
        "\n",
        "# Init Twitter API\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
        "\n",
        "# ------------------------\n",
        "# Step 1: Upload Official Posters\n",
        "# ------------------------\n",
        "num_official = int(input(\"How many official posters do you want to upload? \"))\n",
        "official_posters = []\n",
        "os.makedirs(\"official\", exist_ok=True)\n",
        "\n",
        "print(f\"📤 Please upload {num_official} official poster(s)...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fname in uploaded.keys():\n",
        "    official_posters.append(fname)\n",
        "\n",
        "# Store SHA-256 hashes in ledger\n",
        "ledger = {}\n",
        "for poster in official_posters:\n",
        "    with open(poster, \"rb\") as f:\n",
        "        sha = hashlib.sha256(f.read()).hexdigest()\n",
        "        ledger[poster] = sha\n",
        "\n",
        "with open(\"ledger.json\", \"w\") as f:\n",
        "    json.dump(ledger, f, indent=4)\n",
        "\n",
        "print(\"✅ Official poster hashes stored in ledger.json\")\n",
        "\n",
        "# ------------------------\n",
        "# Step 2: Scrape Candidate Posters from X\n",
        "# ------------------------\n",
        "print(\"🔎 Fetching candidate posters from X...\")\n",
        "query = f\"{MOVIE_NAME} poster has:images -is:retweet\"\n",
        "\n",
        "tweets = client.search_recent_tweets(query=query, max_results=10,\n",
        "                                     tweet_fields=[\"id\",\"created_at\",\"author_id\",\"text\"],\n",
        "                                     expansions=[\"attachments.media_keys\",\"author_id\"],\n",
        "                                     media_fields=[\"url\"])\n",
        "\n",
        "# Map authors\n",
        "author_map = {}\n",
        "if \"users\" in tweets.includes:\n",
        "    for user in tweets.includes[\"users\"]:\n",
        "        author_map[user.id] = user.username\n",
        "\n",
        "# Map media\n",
        "media = {}\n",
        "if \"media\" in tweets.includes:\n",
        "    for m in tweets.includes[\"media\"]:\n",
        "        media[m.media_key] = m\n",
        "\n",
        "os.makedirs(\"candidates\", exist_ok=True)\n",
        "downloaded_candidates = []\n",
        "\n",
        "if tweets.data:\n",
        "    for tweet in tweets.data:\n",
        "        if \"attachments\" in tweet.data:\n",
        "            for mkey in tweet.data[\"attachments\"][\"media_keys\"]:\n",
        "                if mkey in media:\n",
        "                    url = media[mkey].url\n",
        "                    response = requests.get(url)\n",
        "                    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "                    save_path = f\"candidates/{tweet.id}.jpg\"\n",
        "                    img.save(save_path)\n",
        "\n",
        "                    downloaded_candidates.append({\n",
        "                        \"tweet_id\": tweet.id,\n",
        "                        \"username\": author_map.get(tweet.author_id, \"unknown\"),\n",
        "                        \"created_at\": str(tweet.created_at),\n",
        "                        \"text\": tweet.text[:100],  # preview\n",
        "                        \"file\": save_path\n",
        "                    })\n",
        "\n",
        "print(f\"📥 Downloaded {len(downloaded_candidates)} candidate posters.\")\n",
        "\n",
        "# ------------------------\n",
        "# Step 3: CNN Similarity Function\n",
        "# ------------------------\n",
        "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "def get_embedding(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224,224))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    feat = model.predict(x, verbose=0)\n",
        "    return feat.flatten()\n",
        "\n",
        "official_embeddings = [get_embedding(p) for p in official_posters]\n",
        "\n",
        "# ------------------------\n",
        "# Step 4: Verification + Reporting\n",
        "# ------------------------\n",
        "print(\"⚡ Classifying candidate posters...\")\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
        "\n",
        "results = []\n",
        "\n",
        "for cand in downloaded_candidates:\n",
        "    cand_embed = get_embedding(cand[\"file\"])\n",
        "    similarities = [cosine_similarity(cand_embed, off) for off in official_embeddings]\n",
        "    max_sim = max(similarities)\n",
        "    similarity_pct = max_sim * 100\n",
        "\n",
        "    if max_sim < 0.90:\n",
        "        status = \"Irrelevant ⚠️\"\n",
        "    else:\n",
        "        with open(cand[\"file\"], \"rb\") as f:\n",
        "            cand_hash = hashlib.sha256(f.read()).hexdigest()\n",
        "\n",
        "        if cand_hash in ledger.values():\n",
        "            status = \"Official ✅\"\n",
        "        else:\n",
        "            status = \"Fake/Doctored ❌ (Flagged)\"\n",
        "\n",
        "    results.append({\n",
        "        \"Tweet ID\": cand[\"tweet_id\"],\n",
        "        \"Tweet URL\": f\"https://x.com/{cand['username']}/status/{cand['tweet_id']}\",\n",
        "        \"Username\": cand[\"username\"],\n",
        "        \"Posted At\": cand[\"created_at\"],\n",
        "        \"Tweet Snippet\": cand[\"text\"],\n",
        "        \"Similarity (%)\": round(similarity_pct, 2),\n",
        "        \"Classification\": status\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "print(\"\\n📊 Detection Report:\\n\")\n",
        "print(df)\n",
        "\n",
        "# Save report\n",
        "df.to_csv(\"poster_detection_report.csv\", index=False)\n",
        "print(\"\\n✅ Report saved as poster_detection_report.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 33bYPaYgQ8VhnjEMzmD7helvHjg_27GeAa7JV54XGms78V6xY\n",
        "!pip install streamlit pandas pillow pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6Tvpf3Ok_gq",
        "outputId": "025f87e9-a835-483e-ff7c-a91e06ba7ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.50.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.4.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Step 0: Install Required Packages\n",
        "# ================================\n",
        "!pip install tweepy tensorflow pillow pandas streamlit pyngrok tqdm --quiet\n",
        "\n",
        "# ================================\n",
        "# Step 1: Imports\n",
        "# ================================\n",
        "import os, json, hashlib, requests, pandas as pd, numpy as np\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import streamlit as st\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import tweepy\n",
        "\n",
        "# ================================\n",
        "# Step 2: Setup folders\n",
        "# ================================\n",
        "os.makedirs(\"official_posters\", exist_ok=True)\n",
        "os.makedirs(\"candidate_posters\", exist_ok=True)\n",
        "\n",
        "# ================================\n",
        "# Step 3: X API Setup\n",
        "# ================================\n",
        "BEARER_TOKEN = input(\"🔑 Enter your X (Twitter) API Bearer Token: \").strip()\n",
        "MOVIE_NAME = input(\"🎬 Enter the movie name to monitor: \").strip()\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN, wait_on_rate_limit=True)\n",
        "\n",
        "# ================================\n",
        "# Step 4: Upload Official Posters\n",
        "# ================================\n",
        "num_official = int(input(\"How many official posters do you want to upload? \"))\n",
        "print(f\"📤 Please upload {num_official} official poster(s)...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "official_posters = []\n",
        "for fname in uploaded.keys():\n",
        "    os.rename(fname, f\"official_posters/{fname}\")\n",
        "    official_posters.append(f\"official_posters/{fname}\")\n",
        "\n",
        "# Compute SHA-256 hashes\n",
        "ledger = {}\n",
        "for poster_path in official_posters:\n",
        "    with open(poster_path, \"rb\") as f:\n",
        "        sha = hashlib.sha256(f.read()).hexdigest()\n",
        "        ledger[os.path.basename(poster_path)] = sha\n",
        "\n",
        "with open(\"ledger.json\", \"w\") as f:\n",
        "    json.dump(ledger, f, indent=4)\n",
        "print(\"✅ Official poster hashes stored in ledger.json\")\n",
        "\n",
        "# ================================\n",
        "# Step 5: Scrape Candidate Posters from X\n",
        "# ================================\n",
        "query = f\"{MOVIE_NAME} poster has:images -is:retweet\"\n",
        "tweets = client.search_recent_tweets(\n",
        "    query=query,\n",
        "    max_results=50,\n",
        "    tweet_fields=[\"id\",\"created_at\",\"author_id\",\"text\"],\n",
        "    expansions=[\"attachments.media_keys\",\"author_id\"],\n",
        "    media_fields=[\"url\"]\n",
        ")\n",
        "\n",
        "author_map = {user.id:user.username for user in tweets.includes.get(\"users\", [])} if tweets.includes else {}\n",
        "media_map = {m.media_key:m for m in tweets.includes.get(\"media\", [])} if tweets.includes else {}\n",
        "\n",
        "downloaded_candidates = []\n",
        "\n",
        "if tweets.data:\n",
        "    for tweet in tweets.data:\n",
        "        if \"attachments\" in tweet.data:\n",
        "            for mkey in tweet.data[\"attachments\"][\"media_keys\"]:\n",
        "                if mkey in media_map:\n",
        "                    url = media_map[mkey].url\n",
        "                    response = requests.get(url)\n",
        "                    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "                    save_path = f\"candidate_posters/{tweet.id}.jpg\"\n",
        "                    img.save(save_path)\n",
        "                    downloaded_candidates.append({\n",
        "                        \"tweet_id\": tweet.id,\n",
        "                        \"username\": author_map.get(tweet.author_id, \"unknown\"),\n",
        "                        \"created_at\": str(tweet.created_at),\n",
        "                        \"text\": tweet.text[:100],\n",
        "                        \"file\": save_path\n",
        "                    })\n",
        "print(f\"📥 Downloaded {len(downloaded_candidates)} candidate posters.\")\n",
        "\n",
        "# ================================\n",
        "# Step 6: CNN Setup (ResNet50)\n",
        "# ================================\n",
        "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "def get_embedding(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224,224))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    feat = model.predict(x, verbose=0)\n",
        "    return feat.flatten()\n",
        "\n",
        "official_embeddings = [get_embedding(p) for p in official_posters]\n",
        "\n",
        "def cosine_similarity(a,b):\n",
        "    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
        "\n",
        "# ================================\n",
        "# Step 7: Verification + Reporting\n",
        "# ================================\n",
        "results = []\n",
        "\n",
        "for cand in downloaded_candidates:\n",
        "    cand_embed = get_embedding(cand[\"file\"])\n",
        "    similarities = [cosine_similarity(cand_embed, off) for off in official_embeddings]\n",
        "    max_sim = max(similarities)\n",
        "    similarity_pct = max_sim*100\n",
        "\n",
        "    if max_sim < 0.9:\n",
        "        status = \"Irrelevant ⚠️\"\n",
        "    else:\n",
        "        with open(cand[\"file\"], \"rb\") as f:\n",
        "            cand_hash = hashlib.sha256(f.read()).hexdigest()\n",
        "        matched_official = official_posters[np.argmax(similarities)]\n",
        "        matched_name = os.path.basename(matched_official)\n",
        "        if cand_hash == ledger.get(matched_name):\n",
        "            status = \"Official ✅\"\n",
        "        else:\n",
        "            status = \"Fake/Doctored ❌ (Flagged)\"\n",
        "\n",
        "    results.append({\n",
        "        \"Tweet ID\": cand[\"tweet_id\"],\n",
        "        \"Tweet URL\": f\"https://x.com/{cand['username']}/status/{cand['tweet_id']}\",\n",
        "        \"Username\": cand[\"username\"],\n",
        "        \"Posted At\": cand[\"created_at\"],\n",
        "        \"Tweet Snippet\": cand[\"text\"],\n",
        "        \"Similarity (%)\": round(similarity_pct,2),\n",
        "        \"Classification\": status\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"poster_detection_report.csv\", index=False)\n",
        "print(\"✅ Report saved as poster_detection_report.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "1TaJEhQdbjQS",
        "outputId": "2cbefa5b-ac77-45f8-8992-b9728252bf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔑 Enter your X (Twitter) API Bearer Token: AAAAAAAAAAAAAAAAAAAAAFtX4gEAAAAArL%2BLuWsEbVS3mNkfHK3cyfrrfyA%3Dx5C7vZTmkkgYWerzReZ7qi39PFhfBd1DMNeHX9Arzg5Qkgwc8r\n",
            "🎬 Enter the movie name to monitor: Saiyaara\n",
            "How many official posters do you want to upload? 10\n",
            "📤 Please upload 10 official poster(s)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5c567d01-5cf0-47e2-bee2-0dcdd8892481\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5c567d01-5cf0-47e2-bee2-0dcdd8892481\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test 1.jpeg to test 1 (2).jpeg\n",
            "Saving test2.jpg to test2 (2).jpg\n",
            "Saving test3.jpg to test3 (2).jpg\n",
            "Saving test4.jpeg to test4 (2).jpeg\n",
            "Saving test5.jpeg to test5 (2).jpeg\n",
            "Saving test6.jpg to test6 (2).jpg\n",
            "Saving test7.webp to test7 (2).webp\n",
            "Saving test8.jpeg to test8 (2).jpeg\n",
            "Saving test9.jpg to test9 (2).jpg\n",
            "Saving test10.jpg to test10 (2).jpg\n",
            "✅ Official poster hashes stored in ledger.json\n",
            "📥 Downloaded 2 candidate posters.\n",
            "✅ Report saved as poster_detection_report.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Streamlit App for Poster Verification\n",
        "# ================================\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# ================================\n",
        "# 1. Load Report\n",
        "# ================================\n",
        "st.title(\"🎬 Movie Poster Verification Dashboard\")\n",
        "\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    This dashboard shows the classification of candidate posters fetched from X (Twitter)\n",
        "    against the official posters using SHA-256 hashing and ResNet50 embeddings.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"poster_detection_report.csv\")\n",
        "\n",
        "# ================================\n",
        "# 2. Filters\n",
        "# ================================\n",
        "st.sidebar.header(\"Filters\")\n",
        "status_filter = st.sidebar.multiselect(\n",
        "    \"Filter by Classification\",\n",
        "    options=df[\"Classification\"].unique(),\n",
        "    default=df[\"Classification\"].unique()\n",
        ")\n",
        "\n",
        "username_filter = st.sidebar.text_input(\"Filter by Username (optional)\")\n",
        "\n",
        "filtered_df = df[df[\"Classification\"].isin(status_filter)]\n",
        "if username_filter:\n",
        "    filtered_df = filtered_df[filtered_df[\"Username\"].str.contains(username_filter, case=False)]\n",
        "\n",
        "st.write(f\"### Showing {len(filtered_df)} Posters\")\n",
        "\n",
        "# ================================\n",
        "# 3. Display Table\n",
        "# ================================\n",
        "st.dataframe(filtered_df[[\n",
        "    \"Tweet ID\", \"Tweet URL\", \"Username\", \"Posted At\", \"Tweet Snippet\", \"Similarity (%)\", \"Classification\"\n",
        "]])\n",
        "\n",
        "# ================================\n",
        "# 4. Display Images\n",
        "# ================================\n",
        "st.write(\"### Poster Previews\")\n",
        "\n",
        "for idx, row in filtered_df.iterrows():\n",
        "    st.markdown(f\"**{row['Classification']} — @{row['Username']} — {row['Similarity (%)']}% similar**\")\n",
        "    img = Image.open(f\"candidate_posters/{row['Tweet ID']}.jpg\")\n",
        "    st.image(img, use_column_width=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71fGidHJl9S3",
        "outputId": "349ff5e1-531c-48f4-c9bf-1b21cd5bfc0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-10-04 16:02:01.698 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.708 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.715 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.724 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.727 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.729 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.740 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.746 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.755 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.760 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.765 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.774 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.777 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.781 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.784 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.789 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.792 Session state does not function when running a script without `streamlit run`\n",
            "2025-10-04 16:02:01.798 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.801 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.814 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.818 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.819 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.853 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.856 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.858 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.862 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.867 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.870 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.873 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.875 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.879 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.881 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.884 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.885 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.887 The `use_column_width` parameter has been deprecated and will be removed in a future release. Please utilize the `use_container_width` parameter instead.\n",
            "2025-10-04 16:02:01.888 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.962 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.969 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.976 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.989 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.995 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:01.999 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:02.007 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:02.020 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:02.025 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:02.031 The `use_column_width` parameter has been deprecated and will be removed in a future release. Please utilize the `use_container_width` parameter instead.\n",
            "2025-10-04 16:02:02.039 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:02.084 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:02.088 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-10-04 16:02:02.089 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok --quiet\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "!streamlit run app.py &\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmFDs7GkrDVT",
        "outputId": "14e51fec-d29a-42ef-aa85-b178bd7c0cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-10-04T16:11:26+0000 lvl=warn msg=\"failed to open private leg\" id=c28067723c48 privaddr=localhost:8501 err=\"dial tcp 127.0.0.1:8501: connect: connection refused\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-10-04T16:11:26+0000 lvl=warn msg=\"failed to open private leg\" id=52d74ebe2595 privaddr=localhost:8501 err=\"dial tcp 127.0.0.1:8501: connect: connection refused\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-10-04T16:11:30+0000 lvl=warn msg=\"failed to open private leg\" id=0c369933ad8b privaddr=localhost:8501 err=\"dial tcp 127.0.0.1:8501: connect: connection refused\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-10-04T16:11:30+0000 lvl=warn msg=\"failed to open private leg\" id=b4a99901ae75 privaddr=localhost:8501 err=\"dial tcp 127.0.0.1:8501: connect: connection refused\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.231.129.108:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-10-04T16:20:56+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-4cf60be4-6e44-4ef1-b694-3043bf0bd155 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m  Stopping...\u001b[0m\n",
            "NgrokTunnel: \"https://superably-nonargumentative-dotty.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "entire streamlit"
      ],
      "metadata": {
        "id": "-qCalhzxBthd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os, json, hashlib, requests, pandas as pd, numpy as np\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import tweepy\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# ================================\n",
        "# Setup folders\n",
        "# ================================\n",
        "os.makedirs(\"official_posters\", exist_ok=True)\n",
        "os.makedirs(\"candidate_posters\", exist_ok=True)\n",
        "\n",
        "# ================================\n",
        "# Streamlit UI\n",
        "# ================================\n",
        "st.title(\"🎬 Movie Poster Verification Dashboard\")\n",
        "st.markdown(\"\"\"\n",
        "Upload official posters, provide X API credentials, and verify candidate posters from X.\n",
        "\"\"\")\n",
        "\n",
        "# Step 1: Inputs\n",
        "bearer_token = st.text_input(\"Enter your X (Twitter) API Bearer Token\", type=\"password\")\n",
        "movie_name = st.text_input(\"Enter the movie name to monitor\")\n",
        "\n",
        "uploaded_files = st.file_uploader(\n",
        "    \"Upload Official Posters\", type=[\"png\",\"jpg\",\"jpeg\"], accept_multiple_files=True\n",
        ")\n",
        "\n",
        "run_verification = st.button(\"✅ Run Poster Verification\")\n",
        "\n",
        "# ================================\n",
        "# Main Logic\n",
        "# ================================\n",
        "if run_verification:\n",
        "    if not bearer_token or not movie_name or not uploaded_files:\n",
        "        st.error(\"Please provide all inputs and upload at least one official poster.\")\n",
        "    else:\n",
        "        st.info(\"Running verification... This may take a few minutes.\")\n",
        "\n",
        "        # Save official posters and compute hashes\n",
        "        official_posters = []\n",
        "        ledger = {}\n",
        "        for uploaded_file in uploaded_files:\n",
        "            path = os.path.join(\"official_posters\", uploaded_file.name)\n",
        "            with open(path, \"wb\") as f:\n",
        "                f.write(uploaded_file.getbuffer())\n",
        "            official_posters.append(path)\n",
        "\n",
        "            # SHA-256 hash\n",
        "            with open(path, \"rb\") as f:\n",
        "                sha = hashlib.sha256(f.read()).hexdigest()\n",
        "                ledger[uploaded_file.name] = sha\n",
        "\n",
        "        with open(\"ledger.json\", \"w\") as f:\n",
        "            json.dump(ledger, f, indent=4)\n",
        "\n",
        "        # X API setup\n",
        "        client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)\n",
        "\n",
        "        # Scrape candidate posters\n",
        "        query = f\"{movie_name} poster has:images -is:retweet\"\n",
        "        tweets = client.search_recent_tweets(\n",
        "            query=query,\n",
        "            max_results=50,\n",
        "            tweet_fields=[\"id\",\"created_at\",\"author_id\",\"text\"],\n",
        "            expansions=[\"attachments.media_keys\",\"author_id\"],\n",
        "            media_fields=[\"url\"]\n",
        "        )\n",
        "\n",
        "        author_map = {user.id:user.username for user in tweets.includes.get(\"users\", [])} if tweets.includes else {}\n",
        "        media_map = {m.media_key:m for m in tweets.includes.get(\"media\", [])} if tweets.includes else {}\n",
        "\n",
        "        downloaded_candidates = []\n",
        "        if tweets.data:\n",
        "            for tweet in tweets.data:\n",
        "                if \"attachments\" in tweet.data:\n",
        "                    for mkey in tweet.data[\"attachments\"][\"media_keys\"]:\n",
        "                        if mkey in media_map:\n",
        "                            url = media_map[mkey].url\n",
        "                            response = requests.get(url)\n",
        "                            img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "                            save_path = f\"candidate_posters/{tweet.id}.jpg\"\n",
        "                            img.save(save_path)\n",
        "                            downloaded_candidates.append({\n",
        "                                \"tweet_id\": tweet.id,\n",
        "                                \"username\": author_map.get(tweet.author_id, \"unknown\"),\n",
        "                                \"created_at\": str(tweet.created_at),\n",
        "                                \"text\": tweet.text[:100],\n",
        "                                \"file\": save_path\n",
        "                            })\n",
        "\n",
        "        # CNN embeddings\n",
        "        model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "        def get_embedding(img_path):\n",
        "            img = image.load_img(img_path, target_size=(224,224))\n",
        "            x = image.img_to_array(img)\n",
        "            x = np.expand_dims(x, axis=0)\n",
        "            x = preprocess_input(x)\n",
        "            feat = model.predict(x, verbose=0)\n",
        "            return feat.flatten()\n",
        "\n",
        "        official_embeddings = [get_embedding(p) for p in official_posters]\n",
        "        def cosine_similarity(a,b):\n",
        "            return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
        "\n",
        "        # Verification\n",
        "        results = []\n",
        "        for cand in downloaded_candidates:\n",
        "            cand_embed = get_embedding(cand[\"file\"])\n",
        "            similarities = [cosine_similarity(cand_embed, off) for off in official_embeddings]\n",
        "            max_sim = max(similarities)\n",
        "            similarity_pct = max_sim*100\n",
        "\n",
        "            if max_sim < 0.9:\n",
        "                status = \"Irrelevant ⚠️\"\n",
        "            else:\n",
        "                with open(cand[\"file\"], \"rb\") as f:\n",
        "                    cand_hash = hashlib.sha256(f.read()).hexdigest()\n",
        "                matched_official = official_posters[np.argmax(similarities)]\n",
        "                matched_name = os.path.basename(matched_official)\n",
        "                if cand_hash == ledger.get(matched_name):\n",
        "                    status = \"Official ✅\"\n",
        "                else:\n",
        "                    status = \"Fake/Doctored ❌ (Flagged)\"\n",
        "\n",
        "            results.append({\n",
        "                \"Tweet ID\": cand[\"tweet_id\"],\n",
        "                \"Tweet URL\": f\"https://x.com/{cand['username']}/status/{cand['tweet_id']}\",\n",
        "                \"Username\": cand[\"username\"],\n",
        "                \"Posted At\": cand[\"created_at\"],\n",
        "                \"Tweet Snippet\": cand[\"text\"],\n",
        "                \"Similarity (%)\": round(similarity_pct,2),\n",
        "                \"Classification\": status\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(results)\n",
        "        df.to_csv(\"poster_detection_report.csv\", index=False)\n",
        "\n",
        "        st.success(f\"✅ Verification complete! {len(df)} posters analyzed.\")\n",
        "\n",
        "        # Download button\n",
        "        st.download_button(\n",
        "            label=\"📥 Download Report as CSV\",\n",
        "            data=df.to_csv(index=False),\n",
        "            file_name=\"poster_detection_report.csv\",\n",
        "            mime=\"text/csv\"\n",
        "        )\n",
        "\n",
        "        # Display posters\n",
        "        st.write(\"### Poster Previews\")\n",
        "        for idx, row in df.iterrows():\n",
        "            st.markdown(f\"**{row['Classification']} — @{row['Username']} — {row['Similarity (%)']}% similar**\")\n",
        "            img = Image.open(f\"candidate_posters/{row['Tweet ID']}.jpg\")\n",
        "            st.image(img, use_column_width=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3yXt47G_Wfi",
        "outputId": "0c1ad3a7-ad36-4908-8713-e4dcb1c42a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install streamlit pyngrok tensorflow pillow pandas tweepy --quiet\n",
        "\n",
        "# Start Streamlit + ngrok\n",
        "import os, time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "PORT = 8501\n",
        "APP_FILE = \"app.py\"\n",
        "\n",
        "# Start Streamlit in background\n",
        "get_ipython().system_raw(f\"streamlit run {APP_FILE} --server.port {PORT} --server.headless true &\")\n",
        "\n",
        "# Wait for server to be ready\n",
        "time.sleep(10)\n",
        "\n",
        "# Open ngrok tunnel\n",
        "public_url = ngrok.connect(PORT)\n",
        "print(f\"🚀 Streamlit app is live at: {public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8uvZ4mOB0Ba",
        "outputId": "83e45730-2a0b-4d8c-a4bd-8116e52d936e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Streamlit app is live at: NgrokTunnel: \"https://superably-nonargumentative-dotty.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YMp-xJ0dCYa1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}