# -*- coding: utf-8 -*-
"""video_streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UfrtWKHb0P-PxK69EXyeJt89it5yCCC3
"""

# On Colab / Debian-based systems run once:
!apt-get update -y && apt-get install -y ffmpeg -qq

# Python packages (run once)
!pip install streamlit pyngrok tweepy torch torchvision ftfy regex tqdm pillow imagehash librosa soundfile transformers --quiet

# app.py
"""
Video-first Poster/Trailer Verification Streamlit App
-----------------------------------------------------
Features:
- Upload official videos -> ledger.json (sha256, per-frame pHash list, audio MFCC mean)
- Search X for candidate videos (via Tweepy), download them
- Sample frames, compute CLIP frame embeddings (avg pooling), compute similarity to official videos
- pHash & audio MFCC checks -> classification: Official / Repost / Fake / Irrelevant / Unidentified
- Progress bars, previews, downloadable CSV report
Notes:
- Requires ffmpeg installed on system (apt-get install -y ffmpeg)
- Requires Python packages: tweepy, torch, torchvision, transformers, pillow, imagehash, librosa, soundfile
"""

import os
import json
import hashlib
import subprocess
import tempfile
import math
from pathlib import Path
from typing import List, Dict, Any, Tuple
from io import BytesIO

import streamlit as st
import pandas as pd
import numpy as np
from PIL import Image
import imagehash
import requests
from tqdm import tqdm

# audio
import librosa
import soundfile as sf

# torch + CLIP
import torch
import torchvision.transforms as T
from transformers import CLIPProcessor, CLIPModel

# tweepy
import tweepy

# ---------------------------
# Config / constants
# ---------------------------
OFFICIAL_DIR = "official_videos"
CANDIDATE_DIR = "candidate_videos"
FRAME_TMP_DIR = "tmp_frames"
LEDGER_FILE = "ledger.json"
REPORT_FILE = "video_detection_report.csv"

os.makedirs(OFFICIAL_DIR, exist_ok=True)
os.makedirs(CANDIDATE_DIR, exist_ok=True)
os.makedirs(FRAME_TMP_DIR, exist_ok=True)

# threshold tunables
VIS_THRESH = 0.85           # visual embedding cosine threshold
PHASH_HAMMING_THRESH = 10  # acceptable hamming distance for pHash matches (low is better)
PHASH_MATCH_RATE = 0.6     # fraction of frames matching considered good
AUDIO_SIM_THRESH = 0.8     # cosine similarity threshold for audio MFCC mean

# ---------------------------
# Utility functions
# ---------------------------

def compute_sha256(path: str, chunk_size: int = 8192) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            h.update(chunk)
    return h.hexdigest()

def run_ffmpeg_extract_frames(video_path: str, out_dir: str, fps: float = 1.0) -> List[str]:
    """Extract frames using ffmpeg at given fps. Returns list of frame file paths."""
    os.makedirs(out_dir, exist_ok=True)
    # safe path
    out_pattern = os.path.join(out_dir, "frame_%06d.jpg")
    # ffmpeg command: -y overwrite, -i input, -vf fps, -q:v quality
    cmd = [
        "ffmpeg", "-y", "-i", video_path,
        "-vf", f"fps={fps}",
        "-q:v", "2",
        out_pattern
    ]
    # run silently
    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
    frames = sorted([str(p) for p in Path(out_dir).glob("frame_*.jpg")])
    return frames

def compute_phash_for_image(image_path: str) -> str:
    img = Image.open(image_path).convert("RGB")
    return str(imagehash.phash(img))  # hex string

def extract_audio_to_wav(video_path: str, wav_path: str, sr: int = 22050, max_duration: float = 60.0):
    """
    Extract audio portion (first max_duration seconds) using ffmpeg and save to wav_path.
    """
    # Build ffmpeg command extracting up to max_duration seconds
    cmd = [
        "ffmpeg", "-y", "-i", video_path, "-vn",
        "-ac", "1", "-ar", str(sr),
        "-t", str(max_duration),
        wav_path
    ]
    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)

def compute_audio_mfcc_mean(video_path: str, sr: int = 22050, n_mfcc: int = 20, max_duration: float = 60.0) -> List[float]:
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
        wav_path = tmp_wav.name
    try:
        extract_audio_to_wav(video_path, wav_path, sr=sr, max_duration=max_duration)
        y, _ = librosa.load(wav_path, sr=sr, mono=True, duration=max_duration)
        if len(y) == 0:
            return []
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
        mfcc_mean = np.mean(mfcc, axis=1)
        return mfcc_mean.tolist()
    finally:
        try:
            os.remove(wav_path)
        except Exception:
            pass

def phash_hamming_distance(hex1: str, hex2: str) -> int:
    """Compute Hamming distance between two hex pHash strings."""
    # imagehash returns hex-like string; convert to int then xor
    try:
        i1 = int(str(hex1), 16)
        i2 = int(str(hex2), 16)
        x = i1 ^ i2
        return x.bit_count()
    except Exception:
        return 999

# ---------------------------
# CLIP model (frame embeddings)
# ---------------------------
@st.cache_resource(show_spinner=False)
def load_clip_model(device: str = None):
    """Load CLIP model & processor (HuggingFace). Returns (model, processor, device)."""
    # prefer cuda if available
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    model.to(device)
    model.eval()
    return model, processor, device

def compute_clip_frame_embedding(image_path: str, model, processor, device) -> np.ndarray:
    """Return a normalized vector embedding for a single frame jpg using CLIP image encoder."""
    img = Image.open(image_path).convert("RGB")
    inputs = processor(images=img, return_tensors="pt")
    # move tensors to device
    inputs = {k: v.to(device) for k,v in inputs.items()}
    with torch.no_grad():
        outputs = model.get_image_features(**inputs)
    vec = outputs[0].cpu().numpy()
    # L2 normalize
    norm = np.linalg.norm(vec)
    if norm > 0:
        vec = vec / norm
    return vec.flatten()

def avg_pool_frame_embeddings(frame_paths: List[str], model, processor, device, batch_size: int = 8) -> np.ndarray:
    """Compute avg-pooled normalized embedding for a list of frames (batch-friendly)."""
    vecs = []
    # batch processing for speed (but use processor per-image)
    for p in frame_paths:
        try:
            v = compute_clip_frame_embedding(p, model, processor, device)
            vecs.append(v)
        except Exception as e:
            # skip bad frames
            print(f"[warn] clip embedding fail for {p}: {e}")
    if not vecs:
        return np.zeros((512,), dtype=np.float32)
    arr = np.vstack(vecs)
    mean = np.mean(arr, axis=0)
    # normalize
    norm = np.linalg.norm(mean)
    if norm > 0:
        mean = mean / norm
    return mean

def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
    if a is None or b is None:
        return 0.0
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    if denom == 0:
        return 0.0
    return float(np.dot(a, b) / denom)

# ---------------------------
# Ledger / official processing
# ---------------------------

def load_ledger(path: str = LEDGER_FILE) -> Dict[str, Any]:
    if os.path.exists(path):
        try:
            with open(path, "r") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def save_ledger(ledger: Dict[str, Any], path: str = LEDGER_FILE):
    with open(path, "w") as f:
        json.dump(ledger, f, indent=2)

def process_official_video(local_path: str, sampled_fps: float, model, processor, device) -> Dict[str, Any]:
    """
    Processes a single official video: sha256, extract frames & pHashes, audio MFCC mean, frame embeddings (avg).
    Returns metadata dict.
    """
    name = os.path.basename(local_path)
    meta = {}
    meta["path"] = local_path
    meta["sha256"] = compute_sha256(local_path)
    # frames
    tmpdir = tempfile.mkdtemp(prefix="off_frames_")
    frames = run_ffmpeg_extract_frames(local_path, tmpdir, fps=sampled_fps)
    # compute pHashes
    phashes = []
    for f in frames:
        try:
            ph = compute_phash_for_image(f)
            phashes.append(ph)
        except Exception:
            phashes.append(None)
    meta["phashes"] = phashes
    # compute frame-avg CLIP embedding
    emb = avg_pool_frame_embeddings(frames, model, processor, device)
    meta["clip_emb"] = emb.tolist()
    # audio
    audio_mfcc = compute_audio_mfcc_mean(local_path)
    meta["audio_mfcc_mean"] = audio_mfcc
    meta["sampled_fps"] = sampled_fps
    # cleanup frames
    for f in Path(tmpdir).glob("*"):
        try:
            f.unlink()
        except Exception:
            pass
    try:
        Path(tmpdir).rmdir()
    except Exception:
        pass
    return meta

# ---------------------------
# Candidate processing & matching
# ---------------------------

def download_media_from_url(url: str, out_path: str, session: requests.Session = None) -> bool:
    s = session or requests
    try:
        resp = s.get(url, timeout=20)
        resp.raise_for_status()
        with open(out_path, "wb") as f:
            f.write(resp.content)
        return True
    except Exception as e:
        print(f"[warn] download {url} failed: {e}")
        return False

def process_candidate_video(local_path: str, sampled_fps: float, model, processor, device) -> Dict[str, Any]:
    """Process candidate similarly to official but returns only necessary fields."""
    meta = {}
    meta["path"] = local_path
    meta["sha256"] = compute_sha256(local_path)
    tmpdir = tempfile.mkdtemp(prefix="cand_frames_")
    frames = run_ffmpeg_extract_frames(local_path, tmpdir, fps=sampled_fps)
    phashes = []
    for f in frames:
        try:
            ph = compute_phash_for_image(f)
            phashes.append(ph)
        except Exception:
            phashes.append(None)
    meta["phashes"] = phashes
    emb = avg_pool_frame_embeddings(frames, model, processor, device)
    meta["clip_emb"] = emb.tolist()
    audio_mfcc = compute_audio_mfcc_mean(local_path)
    meta["audio_mfcc_mean"] = audio_mfcc
    # leave frames cleanup to caller if preview needed; here we cleanup
    for f in Path(tmpdir).glob("*"):
        try:
            f.unlink()
        except Exception:
            pass
    try:
        Path(tmpdir).rmdir()
    except Exception:
        pass
    return meta

def phash_match_rate(candidate_phashes: List[str], official_phashes: List[str], hamm_thresh: int = PHASH_HAMMING_THRESH) -> float:
    if not candidate_phashes or not official_phashes:
        return 0.0
    matches = 0
    total = 0
    for cp in candidate_phashes:
        if cp is None:
            continue
        total += 1
        # check if any official phash is within hamm_thresh
        found = False
        for op in official_phashes:
            if op is None:
                continue
            dist = phash_hamming_distance(cp, op)
            if dist <= hamm_thresh:
                found = True
                break
        if found:
            matches += 1
    if total == 0:
        return 0.0
    return matches / total

def audio_mfcc_similarity(a: List[float], b: List[float]) -> float:
    if a is None or b is None or len(a)==0 or len(b)==0:
        return 0.0
    a = np.array(a); b = np.array(b)
    # cosine similarity
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    if denom == 0:
        return 0.0
    return float(np.dot(a,b) / denom)

# ---------------------------
# Streamlit App UI
# ---------------------------

st.set_page_config(page_title="Video Verification Dashboard", layout="wide")
st.title("üé• Video Verification ‚Äî Movie / Trailer Authenticity")

st.markdown("""
This app analyzes *videos* (trailers/promos) as official sources and verifies candidate videos fetched from X (Twitter) against the official ledger using:
- SHA-256 (exact file match)
- Per-frame perceptual hashes (pHash)
- Visual embeddings (CLIP on sampled frames, average pooled)
- Audio MFCC similarity
""")

# Sidebar controls
st.sidebar.header("Settings")
sampled_fps = st.sidebar.slider("Sample frames per second (fps)", min_value=0.5, max_value=2.0, value=1.0, step=0.5)
vis_thresh = st.sidebar.slider("Visual similarity threshold", 0.6, 0.99, float(VIS_THRESH), step=0.01)
phash_hamm = st.sidebar.slider("pHash Hamming distance threshold", 0, 64, int(PHASH_HAMMING_THRESH), step=1)
phash_rate_thresh = st.sidebar.slider("pHash match rate threshold (fraction)", 0.1, 1.0, float(PHASH_MATCH_RATE), step=0.05)
audio_sim_thresh = st.sidebar.slider("Audio similarity threshold", 0.1, 1.0, float(AUDIO_SIM_THRESH), step=0.01)

# Inputs
st.subheader("1) Official videos (upload)")
uploaded_offs = st.file_uploader("Upload official video files (trailer, promo). Keep clips short (recommended <60s).", accept_multiple_files=True, type=["mp4","mov","mkv","webm"])
if uploaded_offs:
    if "official_paths" not in st.session_state:
        st.session_state.official_paths = []
    # save uploaded files
    for uf in uploaded_offs:
        save_path = os.path.join(OFFICIAL_DIR, uf.name)
        if save_path not in st.session_state.official_paths:
            with open(save_path, "wb") as f:
                f.write(uf.getbuffer())
            st.session_state.official_paths.append(save_path)
    st.success(f"Saved {len(st.session_state.official_paths)} official videos.")

# option to show current ledger
ledger = load_ledger()
if ledger:
    st.sidebar.markdown(f"**Ledger entries:** {len(ledger)}")
    if st.sidebar.checkbox("Show ledger preview"):
        st.sidebar.json({k: {"sha256": v.get("sha256"), "sampled_fps": v.get("sampled_fps"), "phash_count": len(v.get("phashes",[]))} for k,v in ledger.items()})

# Button to build ledger from uploaded official videos
st.subheader("2) Build / Update Official Ledger")
col1, col2 = st.columns([1,3])
with col1:
    build_btn = st.button("üîß Build/Update Ledger from uploaded videos")
with col2:
    st.write("This computes SHA-256, per-frame pHash, audio MFCC, and CLIP frame embeddings for each uploaded official video and saves them to `ledger.json`.")

if build_btn:
    if not hasattr(st.session_state, "official_paths") or not st.session_state.official_paths:
        st.error("No official videos uploaded yet.")
    else:
        # load CLIP
        with st.spinner("Loading CLIP model..."):
            model, processor, device = load_clip_model()
        ledger = load_ledger()
        progress_bar = st.progress(0)
        total = len(st.session_state.official_paths)
        i = 0
        for p in st.session_state.official_paths:
            i += 1
            st.info(f"Processing official video: {os.path.basename(p)} ({i}/{total})")
            try:
                meta = process_official_video(p, sampled_fps=sampled_fps, model=model, processor=processor, device=device)
                ledger[os.path.basename(p)] = meta
                save_ledger(ledger)
            except Exception as e:
                st.error(f"Failed processing {p}: {e}")
            progress_bar.progress(int(i/total * 100))
        st.success("Ledger build/update complete.")
        st.experimental_rerun()

# Candidate fetching section
st.subheader("3) Fetch candidate videos from X (Twitter)")
bearer_token = st.text_input("X API Bearer Token (v2)", type="password")
movie_query = st.text_input("Search query / movie keywords (e.g. 'MovieName trailer')", value="")
max_tweets = st.number_input("Max tweets to search", min_value=10, max_value=200, value=50, step=10)

fetch_col1, fetch_col2 = st.columns([1,3])
with fetch_col1:
    fetch_btn = st.button("üîç Fetch candidate videos from X")
with fetch_col2:
    st.write("This will search recent tweets with video media matching the query and download the candidate videos to process.")

if fetch_btn:
    if not bearer_token or not movie_query:
        st.error("Provide X API token and search query.")
    else:
        # load CLIP
        with st.spinner("Loading CLIP model..."):
            model, processor, device = load_clip_model()
        # initialize tweepy client
        try:
            client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)
        except Exception as e:
            st.error(f"Tweepy client init failed: {e}")
            client = None
        if client:
            st.info("Searching tweets...")
            try:
                tweets = client.search_recent_tweets(
                    query=f"{movie_query} has:videos -is:retweet",
                    max_results=min(100, int(max_tweets)),
                    expansions=["attachments.media_keys","author_id"],
                    media_fields=["url","type"],
                    tweet_fields=["id","created_at","text","author_id"]
                )
            except Exception as e:
                st.error(f"Search failed: {e}")
                tweets = None
            if tweets and tweets.data:
                # build media map and users
                media_map = {}
                if tweets.includes and "media" in tweets.includes:
                    for m in tweets.includes["media"]:
                        media_map[m.media_key] = m
                users_map = {}
                if tweets.includes and "users" in tweets.includes:
                    for u in tweets.includes["users"]:
                        users_map[u.id] = u.username
                downloaded = []
                sess = requests.Session()
                pbar = st.progress(0)
                total_t = len(tweets.data)
                idx = 0
                for t in tweets.data:
                    idx += 1
                    # guard: attachments may not exist
                    attachments = t.data.get("attachments", {})
                    media_keys = attachments.get("media_keys", []) if attachments else []
                    for mk in media_keys:
                        m = media_map.get(mk)
                        if not m:
                            continue
                        # prefer variants with video url (some media objects are photos)
                        if getattr(m, "type", "") != "video" and getattr(m, "type", "") != "animated_gif":
                            continue
                        # some media objects have 'url' attribute or 'variants' ‚Äî Tweepy v2 may not expose direct URL sometimes.
                        url = getattr(m, "url", None)
                        # fallback: some responses include 'preview_image_url' but not video; in many cases X API provides an m3u8/variant within 'variants' in payload.
                        # For robustness, try to build download url from .url if present; else skip.
                        if not url:
                            # try different repr
                            continue
                        save_name = f"{t.id}.mp4"
                        out_path = os.path.join(CANDIDATE_DIR, save_name)
                        if os.path.exists(out_path):
                            downloaded.append({"tweet": t, "path": out_path})
                        else:
                            ok = download_media_from_url(url, out_path, session=sess)
                            if ok:
                                downloaded.append({"tweet": t, "path": out_path})
                    pbar.progress(int(idx/total_t * 100))
                st.success(f"Downloaded {len(downloaded)} candidate videos to {CANDIDATE_DIR}")
                # store downloaded list in session for processing
                st.session_state.candidates = downloaded
            else:
                st.warning("No tweets found (or API returned no media). Check query and rate-limits.")

# Process candidate videos and classify
st.subheader("4) Process candidates & classify")
process_btn = st.button("‚ñ∂Ô∏è Process & Classify Candidates")

if process_btn:
    if "candidates" not in st.session_state or not st.session_state.candidates:
        st.error("No candidate videos downloaded. Fetch candidates first.")
    else:
        # load CLIP and ledger
        with st.spinner("Loading CLIP & ledger..."):
            model, processor, device = load_clip_model()
            ledger = load_ledger()
        if not ledger:
            st.error("No official ledger found. Build ledger first.")
        else:
            results = []
            total = len(st.session_state.candidates)
            pbar = st.progress(0)
            i = 0
            for item in st.session_state.candidates:
                i += 1
                tweet = item["tweet"]
                cand_path = item["path"]
                st.info(f"Processing candidate tweet {tweet.id} ({i}/{total})")
                try:
                    cand_meta = process_candidate_video(cand_path, sampled_fps=sampled_fps, model=model, processor=processor, device=device)
                except Exception as e:
                    st.error(f"Failed to process candidate {cand_path}: {e}")
                    pbar.progress(int(i/total * 100))
                    continue
                # compare with ledger: compute best visual similarity
                best_sim = -1.0
                best_off = None
                for off_name, off_meta in ledger.items():
                    off_emb = np.array(off_meta.get("clip_emb", []), dtype=np.float32)
                    cand_emb = np.array(cand_meta.get("clip_emb", []), dtype=np.float32)
                    sim = cosine_sim(off_emb, cand_emb)
                    if sim > best_sim:
                        best_sim = sim
                        best_off = (off_name, off_meta)
                classification = "Irrelevant"
                notes = []
                matched_off_name = None
                matched_off_sha = None
                phash_rate = 0.0
                audio_sim = 0.0
                # decide
                if best_sim >= vis_thresh:
                    # we have visually similar content
                    matched_off_name, matched_off_meta = best_off
                    matched_off_sha = matched_off_meta.get("sha256")
                    # compute phash match rate
                    phash_rate = phash_match_rate(cand_meta.get("phashes", []), matched_off_meta.get("phashes", []), hamm_thresh=phash_hamm)
                    # audio sim
                    audio_sim = audio_mfcc_similarity(cand_meta.get("audio_mfcc_mean", []), matched_off_meta.get("audio_mfcc_mean", []))
                    # check exact sha
                    if cand_meta.get("sha256") == matched_off_sha:
                        classification = "Official"
                        notes.append("Exact file SHA-256 match")
                    elif audio_sim >= audio_sim_thresh or phash_rate >= phash_rate_thresh:
                        classification = "Official (Repost / Transcoded)"
                        notes.append(f"Visual sim {best_sim:.3f}, audio_sim {audio_sim:.3f}, phash_rate {phash_rate:.2f}")
                    elif best_sim >= 0.95 and (audio_sim < audio_sim_thresh and phash_rate < phash_rate_thresh):
                        classification = "Fake/Doctored"
                        notes.append(f"Very high visual match ({best_sim:.3f}) but audio/phash mismatch")
                    else:
                        classification = "Unidentified"
                        notes.append(f"Visual sim {best_sim:.3f}; manual review recommended")
                else:
                    classification = "Irrelevant"
                    notes.append(f"Visual sim {best_sim:.3f} < threshold {vis_thresh}")
                # build result row
                result = {
                    "tweet_id": str(tweet.id),
                    "tweet_text": tweet.text[:180] if hasattr(tweet, "text") else "",
                    "username": (tweet.author_id and str(tweet.author_id)) or "",
                    "candidate_path": cand_path,
                    "matched_official": matched_off_name,
                    "visual_similarity": round(float(best_sim), 4) if best_sim is not None else 0.0,
                    "phash_match_rate": round(float(phash_rate), 4),
                    "audio_similarity": round(float(audio_sim), 4),
                    "sha256_match": cand_meta.get("sha256") == matched_off_sha if matched_off_sha else False,
                    "classification": classification,
                    "notes": "; ".join(notes),
                }
                results.append(result)
                pbar.progress(int(i/total * 100))
            # save results
            df = pd.DataFrame(results)
            df.to_csv(REPORT_FILE, index=False)
            st.success(f"Processing complete. {len(df)} candidates classified. Report saved to {REPORT_FILE}")
            st.dataframe(df)
            st.download_button("üì• Download CSV report", data=df.to_csv(index=False), file_name=REPORT_FILE, mime="text/csv")
            # display previews for flagged items
            flagged = df[df["classification"].isin(["Fake/Doctored","Unidentified","Official (Repost / Transcoded)"])]
            if not flagged.empty:
                st.subheader("Flagged / Interesting Candidates")
                for idx, row in flagged.iterrows():
                    st.write(f"**{row['classification']}** ‚Äî tweet: {row['tweet_id']} ‚Äî visual_sim {row['visual_similarity']} ‚Äî phash_rate {row['phash_match_rate']} ‚Äî audio_sim {row['audio_similarity']}")
                    # show small video preview
                    try:
                        st.video(row["candidate_path"])
                    except Exception:
                        st.write("Preview not available")

# ===============================
# 0Ô∏è‚É£ Install packages
# ===============================
!apt-get install -y ffmpeg --quiet
!pip install streamlit pyngrok tweepy torch torchvision torchaudio pillow pandas tqdm librosa soundfile transformers imagehash --quiet

!pip install streamlit pyngrok tensorflow pillow pandas tweepy --quiet

import os, time
from pyngrok import ngrok

# ===============================
# Install required packages
# ===============================
!sudo apt-get update -qq
!sudo apt-get install -y ffmpeg openssh-client
!pip install streamlit tweepy torch torchvision torchaudio pillow pandas tqdm librosa soundfile transformers imagehash pyngrok --quiet

import os

os.makedirs("official_posters", exist_ok=True)
os.makedirs("candidate_posters", exist_ok=True)
os.makedirs("official_videos", exist_ok=True)
os.makedirs("candidate_videos", exist_ok=True)

from pyngrok import ngrok

NGROK_TOKEN = "33c6zLu0TJnlczZmzaMZF8GbwlA_5d1kftwFyEEHnZM8Z3wJa"
ngrok.set_auth_token(NGROK_TOKEN)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile video_app.py
# import os, json, hashlib, subprocess, tempfile
# from pathlib import Path
# from typing import List, Dict, Any
# 
# import streamlit as st
# import pandas as pd
# import numpy as np
# from PIL import Image
# import imagehash
# import requests
# import librosa
# import torch
# from transformers import CLIPProcessor, CLIPModel
# import tweepy
# from pyngrok import ngrok
# 
# # ---------------------------
# # Ngrok setup
# # ---------------------------
# NGROK_TOKEN = "33c6zLu0TJnlczZmzaMZF8GbwlA_5d1kftwFyEEHnZM8Z3wJa"
# ngrok.set_auth_token(NGROK_TOKEN)
# PORT = 8505
# public_url = ngrok.connect(PORT)
# st.write(f"üöÄ Streamlit app live at: {public_url}")
# 
# # ---------------------------
# # Config / folders
# # ---------------------------
# OFFICIAL_DIR = "official_videos"
# CANDIDATE_DIR = "candidate_videos"
# FRAME_TMP_DIR = "tmp_frames"
# LEDGER_FILE = "ledger.json"
# REPORT_FILE = "video_detection_report.csv"
# 
# os.makedirs(OFFICIAL_DIR, exist_ok=True)
# os.makedirs(CANDIDATE_DIR, exist_ok=True)
# os.makedirs(FRAME_TMP_DIR, exist_ok=True)
# 
# VIS_THRESH = 0.85
# PHASH_HAMMING_THRESH = 10
# PHASH_MATCH_RATE = 0.6
# AUDIO_SIM_THRESH = 0.8
# 
# # ---------------------------
# # Utilities
# # ---------------------------
# def compute_sha256(path: str) -> str:
#     h = hashlib.sha256()
#     with open(path, "rb") as f:
#         for chunk in iter(lambda: f.read(8192), b""):
#             h.update(chunk)
#     return h.hexdigest()
# 
# def run_ffmpeg_extract_frames(video_path: str, out_dir: str, fps: float = 1.0) -> List[str]:
#     os.makedirs(out_dir, exist_ok=True)
#     out_pattern = os.path.join(out_dir, "frame_%06d.jpg")
#     cmd = ["ffmpeg","-y","-i",video_path,"-vf",f"fps={fps}","-q:v","2",out_pattern]
#     subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
#     return sorted([str(p) for p in Path(out_dir).glob("frame_*.jpg")])
# 
# def compute_phash_for_image(image_path: str) -> str:
#     img = Image.open(image_path).convert("RGB")
#     return str(imagehash.phash(img))
# 
# def extract_audio_to_wav(video_path: str, wav_path: str, sr: int = 22050, max_duration: float = 60.0):
#     cmd = ["ffmpeg","-y","-i",video_path,"-vn","-ac","1","-ar",str(sr),"-t",str(max_duration),wav_path]
#     subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
# 
# def compute_audio_mfcc_mean(video_path: str) -> List[float]:
#     with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
#         wav_path = tmp_wav.name
#     try:
#         extract_audio_to_wav(video_path, wav_path)
#         y, _ = librosa.load(wav_path, sr=22050, mono=True, duration=60.0)
#         if len(y)==0:
#             return []
#         mfcc = librosa.feature.mfcc(y=y, sr=22050, n_mfcc=20)
#         return np.mean(mfcc, axis=1).tolist()
#     finally:
#         try: os.remove(wav_path)
#         except: pass
# 
# def phash_hamming_distance(hex1: str, hex2: str) -> int:
#     try: return (int(hex1,16) ^ int(hex2,16)).bit_count()
#     except: return 999
# 
# def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
#     if a is None or b is None: return 0.0
#     denom = np.linalg.norm(a)*np.linalg.norm(b)
#     if denom==0: return 0.0
#     return float(np.dot(a,b)/denom)
# 
# # ---------------------------
# # CLIP Model
# # ---------------------------
# @st.cache_resource(show_spinner=False)
# def load_clip_model(device: str = None):
#     if device is None:
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#     model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
#     processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
#     model.to(device)
#     model.eval()
#     return model, processor, device
# 
# def compute_clip_frame_embedding(image_path: str, model, processor, device) -> np.ndarray:
#     img = Image.open(image_path).convert("RGB")
#     inputs = processor(images=img, return_tensors="pt")
#     inputs = {k:v.to(device) for k,v in inputs.items()}
#     with torch.no_grad():
#         outputs = model.get_image_features(**inputs)
#     vec = outputs[0].cpu().numpy()
#     norm = np.linalg.norm(vec)
#     if norm>0: vec = vec/norm
#     return vec.flatten()
# 
# def avg_pool_frame_embeddings(frame_paths: List[str], model, processor, device) -> np.ndarray:
#     vecs=[]
#     for p in frame_paths:
#         try: vecs.append(compute_clip_frame_embedding(p, model, processor, device))
#         except: continue
#     if not vecs: return np.zeros((512,),dtype=np.float32)
#     mean = np.mean(np.vstack(vecs), axis=0)
#     norm = np.linalg.norm(mean)
#     if norm>0: mean=mean/norm
#     return mean
# 
# # ---------------------------
# # Ledger functions
# # ---------------------------
# def load_ledger(path: str = LEDGER_FILE) -> Dict[str, Any]:
#     if os.path.exists(path):
#         try:
#             with open(path,"r") as f: return json.load(f)
#         except: return {}
#     return {}
# 
# def save_ledger(ledger: Dict[str, Any], path: str = LEDGER_FILE):
#     with open(path,"w") as f: json.dump(ledger,f,indent=2)
# 
# # ---------------------------
# # Streamlit UI
# # ---------------------------
# st.set_page_config(page_title="Video Verification Dashboard", layout="wide")
# st.title("üé• Video Verification ‚Äî Movie / Trailer Authenticity")
# 
# # Sidebar
# st.sidebar.header("Settings")
# sampled_fps = st.sidebar.slider("Sample frames per second (fps)",0.5,2.0,1.0,0.5)
# vis_thresh = st.sidebar.slider("Visual similarity threshold",0.6,0.99,VIS_THRESH,0.01)
# phash_hamm = st.sidebar.slider("pHash Hamming distance threshold",0,64,PHASH_HAMMING_THRESH,1)
# phash_rate_thresh = st.sidebar.slider("pHash match rate threshold",0.1,1.0,PHASH_MATCH_RATE,0.05)
# audio_sim_thresh = st.sidebar.slider("Audio similarity threshold",0.1,1.0,AUDIO_SIM_THRESH,0.01)
# 
# # ---------------------------
# # Upload Official Videos
# # ---------------------------
# st.subheader("1Ô∏è‚É£ Upload Official Videos")
# uploaded_offs = st.file_uploader("Upload official videos (trailer, promo)", accept_multiple_files=True, type=["mp4","mov","mkv","webm"])
# if uploaded_offs:
#     if "official_paths" not in st.session_state: st.session_state.official_paths=[]
#     for uf in uploaded_offs:
#         save_path=os.path.join(OFFICIAL_DIR,uf.name)
#         if save_path not in st.session_state.official_paths:
#             with open(save_path,"wb") as f: f.write(uf.getbuffer())
#             st.session_state.official_paths.append(save_path)
#     st.success(f"Saved {len(st.session_state.official_paths)} official videos.")
# 
# # ---------------------------
# # Build / Update Ledger
# # ---------------------------
# st.subheader("2Ô∏è‚É£ Build / Update Ledger")
# build_btn = st.button("üîß Build/Update Ledger from uploaded videos")
# 
# if build_btn:
#     if not hasattr(st.session_state,"official_paths") or not st.session_state.official_paths:
#         st.error("No official videos uploaded.")
#     else:
#         with st.spinner("Loading CLIP model..."):
#             model, processor, device = load_clip_model()
#         ledger = load_ledger()
#         total = len(st.session_state.official_paths)
#         for i,p in enumerate(st.session_state.official_paths,1):
#             st.info(f"Processing official video: {os.path.basename(p)} ({i}/{total})")
#             tmpdir = tempfile.mkdtemp(prefix="off_frames_")
#             frames = run_ffmpeg_extract_frames(p,tmpdir,fps=sampled_fps)
#             phashes=[compute_phash_for_image(f) for f in frames]
#             emb=avg_pool_frame_embeddings(frames,model,processor,device)
#             audio_mfcc=compute_audio_mfcc_mean(p)
#             ledger[os.path.basename(p)]={"path":p,"sha256":compute_sha256(p),"phashes":phashes,"clip_emb":emb.tolist(),"audio_mfcc_mean":audio_mfcc,"sampled_fps":sampled_fps}
#             for f in Path(tmpdir).glob("*"): f.unlink()
#             try: Path(tmpdir).rmdir()
#             except: pass
#         save_ledger(ledger)
#         st.success("Ledger build/update complete.")
# 
# # ---------------------------
# # Fetch candidate videos from X
# # ---------------------------
# st.subheader("3Ô∏è‚É£ Fetch candidate videos from X")
# bearer_token = st.text_input("Enter X API Bearer Token (for candidate videos)")
# movie_query = st.text_input("Movie / Trailer search query")
# fetch_btn = st.button("‚ñ∂Ô∏è Fetch candidate videos")
# 
# if fetch_btn:
#     if not bearer_token or not movie_query:
#         st.error("Provide both X API token and query")
#     else:
#         client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)
#         query = f"{movie_query} has:videos -is:retweet"
#         tweets = client.search_recent_tweets(
#             query=query,
#             max_results=30,
#             expansions=["attachments.media_keys","author_id"],
#             media_fields=["url","type","duration_ms"],
#             tweet_fields=["id","author_id","created_at","text"]
#         )
#         authors = {u.id:u.username for u in tweets.includes.get("users",[])} if tweets.includes else {}
#         media_map = {m.media_key:m for m in tweets.includes.get("media",[])} if tweets.includes else {}
# 
#         downloaded=[]
#         if tweets.data:
#             for t in tweets.data:
#                 if "attachments" in t.data:
#                     for mk in t.data["attachments"]["media_keys"]:
#                         if mk in media_map and media_map[mk].type=="video":
#                             url = media_map[mk].url
#                             resp = requests.get(url)
#                             save_path=os.path.join(CANDIDATE_DIR,f"{t.id}.mp4")
#                             with open(save_path,"wb") as f: f.write(resp.content)
#                             downloaded.append({"tweet_id":t.id,"username":authors.get(t.author_id,"unknown"),"created_at":str(t.created_at),"text":t.text[:100],"file":save_path})
#         st.session_state.candidates = downloaded
#         st.success(f"Downloaded {len(downloaded)} candidate videos")
# 
# # ---------------------------
# # Verification / classification
# # ---------------------------
# verify_btn = st.button("‚úÖ Run Verification")
# 
# if verify_btn:
#     if "candidates" not in st.session_state or not st.session_state.candidates:
#         st.error("No candidate videos available for verification")
#     else:
#         st.info("Running verification...")
#         with st.spinner("Loading CLIP model..."):
#             model, processor, device = load_clip_model()
#         ledger = load_ledger()
#         results=[]
#         for cand in st.session_state.candidates:
#             try:
#                 frames=run_ffmpeg_extract_frames(cand["file"],FRAME_TMP_DIR,fps=sampled_fps)
#                 cand_phashes=[compute_phash_for_image(f) for f in frames]
#                 phash_matches=[]
#                 for off_name, off_data in ledger.items():
#                     total=len(off_data["phashes"])
#                     match_count=sum(phash_hamming_distance(p1,p2)<=phash_hamm for p1,p2 in zip(cand_phashes,off_data["phashes"]))
#                     if total>0: phash_matches.append(match_count/total)
#                     else: phash_matches.append(0.0)
#                 phash_max=max(phash_matches) if phash_matches else 0.0
# 
#                 cand_emb=avg_pool_frame_embeddings(frames,model,processor,device)
#                 clip_sims=[cosine_sim(cand_emb,np.array(off_data["clip_emb"])) for off_data in ledger.values()]
#                 clip_max=max(clip_sims) if clip_sims else 0.0
# 
#                 cand_audio=compute_audio_mfcc_mean(cand["file"])
#                 audio_sims=[]
#                 for off_data in ledger.values():
#                     if off_data["audio_mfcc_mean"] and cand_audio:
#                         sim=cosine_sim(np.array(off_data["audio_mfcc_mean"]),np.array(cand_audio))
#                         audio_sims.append(sim)
#                     else: audio_sims.append(0.0)
#                 audio_max=max(audio_sims) if audio_sims else 0.0
# 
#                 classification="Irrelevant ‚ö†Ô∏è"
#                 if clip_max>vis_thresh:
#                     if phash_max>phash_rate_thresh:
#                         cand_sha=compute_sha256(cand["file"])
#                         matched_off_name=list(ledger.keys())[np.argmax(clip_sims)]
#                         if cand_sha==ledger[matched_off_name]["sha256"]:
#                             classification="Official ‚úÖ"
#                         else:
#                             classification="Fake/Doctored ‚ùå"
#                     else:
#                         classification="Repost üîÅ"
# 
#                 results.append({"Tweet ID":cand["tweet_id"],
#                                 "Tweet URL":f"https://x.com/{cand['username']}/status/{cand['tweet_id']}",
#                                 "Username":cand["username"],
#                                 "Posted At":cand["created_at"],
#                                 "Tweet Snippet":cand["text"],
#                                 "pHash Match":round(phash_max*100,2),
#                                 "CLIP Sim (%)":round(clip_max*100,2),
#                                 "Audio Sim (%)":round(audio_max*100,2),
#                                 "Classification":classification})
# 
#             except Exception as e:
#                 st.warning(f"Failed {cand['file']}: {e}")
#         df=pd.DataFrame(results)
#         df.to_csv(REPORT_FILE,index=False)
#         st.success(f"‚úÖ Verification complete! {len(df)} videos analyzed.")
#         st.download_button("üì• Download Report CSV", df.to_csv(index=False), file_name=REPORT_FILE, mime="text/csv")
#         st.write("### Candidate Video Previews")
#         for row in results:
#             st.markdown(f"**{row['Classification']} ‚Äî @{row['Username']} ‚Äî CLIP {row['CLIP Sim (%)']}%**")
#

import subprocess
from pyngrok import ngrok

PORT = 8505

# Start Streamlit in background
subprocess.Popen(["streamlit", "run", "video_app.py", "--server.port", str(PORT)])

# Expose it via ngrok
public_url = ngrok.connect(PORT)
print(f"Streamlit app live at: {public_url}")

!pip install --quiet streamlit pyngrok tweepy pillow pandas numpy imagehash librosa torch torchvision transformers
!apt-get install -qq ffmpeg

import os, json, hashlib, subprocess, tempfile
from pathlib import Path
from typing import List, Dict, Any

import streamlit as st
import pandas as pd
import numpy as np
from PIL import Image
import imagehash
import requests
import librosa
import torch
from transformers import CLIPProcessor, CLIPModel
import tweepy
from pyngrok import ngrok

# ---------------------------
# Ngrok setup
# ---------------------------
NGROK_TOKEN = "33c6zLu0TJnlczZmzaMZF8GbwlA_5d1kftwFyEEHnZM8Z3wJa"
ngrok.set_auth_token(NGROK_TOKEN)
PORT = 8505
public_url = ngrok.connect(PORT)
print(f"üöÄ Streamlit app live at: {public_url}")

from pyngrok import ngrok

# List all active tunnels
tunnels = ngrok.get_tunnels()
print(tunnels)

# Kill all tunnels (or target specific one)
ngrok.kill()  # kills all tunnels

public_url = ngrok.connect(PORT)
print(f"Streamlit app live at: {public_url}")

"""# **FINAL**"""

!pip install streamlit pyngrok tweepy transformers torch torchvision torchaudio pillow imagehash librosa pandas numpy requests
!apt-get install -y ffmpeg

# Commented out IPython magic to ensure Python compatibility.
# %%writefile video_app.py
# import os, json, hashlib, subprocess, tempfile
# from pathlib import Path
# from typing import List, Dict, Any
# 
# import streamlit as st
# import pandas as pd
# import numpy as np
# from PIL import Image
# import imagehash
# import requests
# import librosa
# import torch
# from transformers import CLIPProcessor, CLIPModel
# import tweepy
# from pyngrok import ngrok
# 
# # ---------------------------
# # Ngrok setup
# # ---------------------------
# NGROK_TOKEN = "33c6zLu0TJnlczZmzaMZF8GbwlA_5d1kftwFyEEHnZM8Z3wJa"
# ngrok.set_auth_token(NGROK_TOKEN)
# PORT = 8505
# public_url = ngrok.connect(PORT)
# st.write(f"üöÄ Streamlit app live at: {public_url}")
# 
# # ---------------------------
# # Config / folders
# # ---------------------------
# OFFICIAL_DIR = "official_videos"
# CANDIDATE_DIR = "candidate_videos"
# FRAME_TMP_DIR = "tmp_frames"
# LEDGER_FILE = "ledger.json"
# REPORT_FILE = "video_detection_report.csv"
# 
# os.makedirs(OFFICIAL_DIR, exist_ok=True)
# os.makedirs(CANDIDATE_DIR, exist_ok=True)
# os.makedirs(FRAME_TMP_DIR, exist_ok=True)
# 
# VIS_THRESH = 0.85
# PHASH_HAMMING_THRESH = 10
# PHASH_MATCH_RATE = 0.6
# AUDIO_SIM_THRESH = 0.8
# 
# # ---------------------------
# # Utilities
# # ---------------------------
# def compute_sha256(path: str) -> str:
#     h = hashlib.sha256()
#     with open(path, "rb") as f:
#         for chunk in iter(lambda: f.read(8192), b""):
#             h.update(chunk)
#     return h.hexdigest()
# 
# def run_ffmpeg_extract_frames(video_path: str, out_dir: str, fps: float = 1.0) -> List[str]:
#     os.makedirs(out_dir, exist_ok=True)
#     out_pattern = os.path.join(out_dir, "frame_%06d.jpg")
#     cmd = ["ffmpeg","-y","-i",video_path,"-vf",f"fps={fps}","-q:v","2",out_pattern]
#     subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
#     return sorted([str(p) for p in Path(out_dir).glob("frame_*.jpg")])
# 
# def compute_phash_for_image(image_path: str) -> str:
#     img = Image.open(image_path).convert("RGB")
#     return str(imagehash.phash(img))
# 
# def extract_audio_to_wav(video_path: str, wav_path: str, sr: int = 22050, max_duration: float = 60.0):
#     cmd = ["ffmpeg","-y","-i",video_path,"-vn","-ac","1","-ar",str(sr),"-t",str(max_duration),wav_path]
#     subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
# 
# def compute_audio_mfcc_mean(video_path: str) -> List[float]:
#     with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
#         wav_path = tmp_wav.name
#     try:
#         extract_audio_to_wav(video_path, wav_path)
#         y, _ = librosa.load(wav_path, sr=22050, mono=True, duration=60.0)
#         if len(y)==0: return []
#         mfcc = librosa.feature.mfcc(y=y, sr=22050, n_mfcc=20)
#         return np.mean(mfcc, axis=1).tolist()
#     finally:
#         try: os.remove(wav_path)
#         except: pass
# 
# def phash_hamming_distance(hex1: str, hex2: str) -> int:
#     try: return (int(hex1,16) ^ int(hex2,16)).bit_count()
#     except: return 999
# 
# def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
#     if a is None or b is None: return 0.0
#     denom = np.linalg.norm(a)*np.linalg.norm(b)
#     if denom==0: return 0.0
#     return float(np.dot(a,b)/denom)
# 
# # ---------------------------
# # CLIP Model
# # ---------------------------
# @st.cache_resource(show_spinner=False)
# def load_clip_model(device: str = None):
#     if device is None:
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#     model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
#     processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
#     model.to(device)
#     model.eval()
#     return model, processor, device
# 
# def compute_clip_frame_embedding(image_path: str, model, processor, device) -> np.ndarray:
#     img = Image.open(image_path).convert("RGB")
#     inputs = processor(images=img, return_tensors="pt")
#     inputs = {k:v.to(device) for k,v in inputs.items()}
#     with torch.no_grad():
#         outputs = model.get_image_features(**inputs)
#     vec = outputs[0].cpu().numpy()
#     norm = np.linalg.norm(vec)
#     if norm>0: vec = vec/norm
#     return vec.flatten()
# 
# def avg_pool_frame_embeddings(frame_paths: List[str], model, processor, device) -> np.ndarray:
#     vecs=[]
#     for p in frame_paths:
#         try: vecs.append(compute_clip_frame_embedding(p, model, processor, device))
#         except: continue
#     if not vecs: return np.zeros((512,),dtype=np.float32)
#     mean = np.mean(np.vstack(vecs), axis=0)
#     norm = np.linalg.norm(mean)
#     if norm>0: mean=mean/norm
#     return mean
# 
# # ---------------------------
# # Ledger functions
# # ---------------------------
# def load_ledger(path: str = LEDGER_FILE) -> Dict[str, Any]:
#     if os.path.exists(path):
#         try:
#             with open(path,"r") as f: return json.load(f)
#         except: return {}
#     return {}
# 
# def save_ledger(ledger: Dict[str, Any], path: str = LEDGER_FILE):
#     with open(path,"w") as f: json.dump(ledger,f,indent=2)
# 
# # ---------------------------
# # Streamlit UI
# # ---------------------------
# st.set_page_config(page_title="Video Verification Dashboard", layout="wide")
# st.title("üé• Video Verification ‚Äî Movie / Trailer Authenticity")
# 
# # Sidebar
# st.sidebar.header("Settings")
# sampled_fps = st.sidebar.slider("Sample frames per second (fps)",0.5,2.0,1.0,0.5)
# vis_thresh = st.sidebar.slider("Visual similarity threshold",0.6,0.99,VIS_THRESH,0.01)
# phash_hamm = st.sidebar.slider("pHash Hamming distance threshold",0,64,PHASH_HAMMING_THRESH,1)
# phash_rate_thresh = st.sidebar.slider("pHash match rate threshold",0.1,1.0,PHASH_MATCH_RATE,0.05)
# audio_sim_thresh = st.sidebar.slider("Audio similarity threshold",0.1,1.0,AUDIO_SIM_THRESH,0.01)
# 
# # ---------------------------
# # 1Ô∏è‚É£ Upload Official Videos
# # ---------------------------
# st.subheader("1Ô∏è‚É£ Upload Official Videos")
# uploaded_offs = st.file_uploader(
#     "Upload official videos (trailer, promo)",
#     accept_multiple_files=True,
#     type=["mp4","mov","mkv","webm"]
# )
# if uploaded_offs:
#     if "official_paths" not in st.session_state: st.session_state.official_paths=[]
#     for uf in uploaded_offs:
#         save_path=os.path.join(OFFICIAL_DIR,uf.name)
#         if save_path not in st.session_state.official_paths:
#             with open(save_path,"wb") as f: f.write(uf.getbuffer())
#             st.session_state.official_paths.append(save_path)
#     st.success(f"Saved {len(st.session_state.official_paths)} official videos.")
# 
# # ---------------------------
# # 2Ô∏è‚É£ Build / Update Ledger
# # ---------------------------
# st.subheader("2Ô∏è‚É£ Build / Update Ledger")
# build_btn = st.button("üîß Build/Update Ledger from uploaded videos")
# 
# if build_btn:
#     if not hasattr(st.session_state,"official_paths") or not st.session_state.official_paths:
#         st.error("No official videos uploaded.")
#     else:
#         with st.spinner("Loading CLIP model..."):
#             model, processor, device = load_clip_model()
#         ledger = load_ledger()
#         total = len(st.session_state.official_paths)
#         for i,p in enumerate(st.session_state.official_paths,1):
#             st.info(f"Processing official video: {os.path.basename(p)} ({i}/{total})")
#             tmpdir = tempfile.mkdtemp(prefix="off_frames_")
#             frames = run_ffmpeg_extract_frames(p,tmpdir,fps=sampled_fps)
#             phashes=[compute_phash_for_image(f) for f in frames]
#             emb=avg_pool_frame_embeddings(frames,model,processor,device)
#             audio_mfcc=compute_audio_mfcc_mean(p)
#             ledger[os.path.basename(p)]={"path":p,"sha256":compute_sha256(p),
#                                          "phashes":phashes,"clip_emb":emb.tolist(),
#                                          "audio_mfcc_mean":audio_mfcc,"sampled_fps":sampled_fps}
#             for f in Path(tmpdir).glob("*"): f.unlink()
#             try: Path(tmpdir).rmdir()
#             except: pass
#         save_ledger(ledger)
#         st.success("Ledger build/update complete.")
# 
# # ---------------------------
# # 3Ô∏è‚É£ Fetch candidate videos from X
# # ---------------------------
# st.subheader("3Ô∏è‚É£ Fetch candidate videos from X")
# bearer_token = st.text_input("Enter X API Bearer Token (for candidate videos)")
# movie_query = st.text_input("Movie / Trailer search query")
# fetch_btn = st.button("‚ñ∂Ô∏è Fetch candidate videos")
# 
# if fetch_btn:
#     if not bearer_token or not movie_query:
#         st.error("Provide both X API token and query")
#     else:
#         try:
#             client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)
#             query = f"{movie_query} has:videos -is:retweet"
#             tweets = client.search_recent_tweets(
#                 query=query,
#                 max_results=30,
#                 expansions=["attachments.media_keys","author_id"],
#                 media_fields=["variants","type","duration_ms"],
#                 tweet_fields=["id","author_id","created_at","text"]
#             )
# 
#             authors = {u.id:u.username for u in tweets.includes.get("users",[])} if tweets.includes else {}
#             media_map = {m.media_key:m for m in tweets.includes.get("media",[])} if tweets.includes else {}
# 
#             downloaded=[]
#             if tweets.data:
#                 for t in tweets.data:
#                     if "attachments" in t.data:
#                         for mk in t.data["attachments"]["media_keys"]:
#                             if mk in media_map and media_map[mk].type=="video":
#                                 media_obj = media_map[mk]
#                                 video_url = None
#                                 for var in getattr(media_obj, "variants", []):
#                                     if var.get("content_type")=="video/mp4":
#                                         video_url = var.get("url")
#                                 if video_url:
#                                     resp = requests.get(video_url)
#                                     save_path=os.path.join(CANDIDATE_DIR,f"{t.id}.mp4")
#                                     with open(save_path,"wb") as f:
#                                         f.write(resp.content)
#                                     downloaded.append({
#                                         "tweet_id": t.id,
#                                         "username": authors.get(t.author_id,"unknown"),
#                                         "created_at": str(t.created_at),
#                                         "text": t.text[:100],
#                                         "file": save_path
#                                     })
#             st.session_state.candidates = downloaded
#             st.success(f"Downloaded {len(downloaded)} candidate videos")
#         except tweepy.errors.Unauthorized:
#             st.error("‚ö†Ô∏è Unauthorized: check your X API bearer token")
#         except Exception as e:
#             st.error(f"‚ö†Ô∏è Error fetching candidate videos: {e}")
# 
# # ---------------------------
# # 4Ô∏è‚É£ Verification / Classification
# # ---------------------------
# verify_btn = st.button("‚úÖ Run Verification")
# 
# if verify_btn:
#     if "candidates" not in st.session_state or not st.session_state.candidates:
#         st.error("No candidate videos available for verification")
#     else:
#         st.info("Running verification...")
#         with st.spinner("Loading CLIP model..."):
#             model, processor, device = load_clip_model()
#         ledger = load_ledger()
#         results=[]
#         for cand in st.session_state.candidates:
#             try:
#                 frames=run_ffmpeg_extract_frames(cand["file"],FRAME_TMP_DIR,fps=sampled_fps)
#                 cand_phashes=[compute_phash_for_image(f) for f in frames]
#                 phash_matches=[]
#                 for off_data in ledger.values():
#                     total=len(off_data["phashes"])
#                     match_count=sum(phash_hamming_distance(p1,p2)<=phash_hamm for p1,p2 in zip(cand_phashes,off_data["phashes"]))
#                     if total>0: phash_matches.append(match_count/total)
#                     else: phash_matches.append(0.0)
#                 phash_max=max(phash_matches) if phash_matches else 0.0
# 
#                 cand_emb=avg_pool_frame_embeddings(frames,model,processor,device)
#                 clip_sims=[cosine_sim(cand_emb,np.array(off_data["clip_emb"])) for off_data in ledger.values()]
#                 clip_max=max(clip_sims) if clip_sims else 0.0
# 
#                 cand_audio=compute_audio_mfcc_mean(cand["file"])
#                 audio_sims=[]
#                 for off_data in ledger.values():
#                     if off_data["audio_mfcc_mean"] and cand_audio:
#                         sim=cosine_sim(np.array(off_data["audio_mfcc_mean"]),np.array(cand_audio))
#                         audio_sims.append(sim)
#                     else: audio_sims.append(0.0)
#                 audio_max=max(audio_sims) if audio_sims else 0.0
# 
#                 classification="Irrelevant ‚ö†Ô∏è"
#                 if clip_max>vis_thresh:
#                     if phash_max>phash_rate_thresh:
#                         cand_sha=compute_sha256(cand["file"])
#                         matched_off_name=list(ledger.keys())[np.argmax(clip_sims)]
#                         if cand_sha==ledger[matched_off_name]["sha256"]:
#                             classification="Official ‚úÖ"
#                         else:
#                             classification="Fake/Doctored ‚ùå"
#                     else:
#                         classification="Repost üîÅ"
# 
#                 results.append({"Tweet ID":cand["tweet_id"],
#                                 "Tweet URL":f"https://x.com/{cand['username']}/status/{cand['tweet_id']}",
#                                 "Username":cand["username"],
#                                 "Posted At":cand["created_at"],
#                                 "Tweet Snippet":cand["text"],
#                                 "pHash Match":round(phash_max*100,2),
#                                 "CLIP Sim (%)":round(clip_max*100,2),
#                                 "Audio Sim (%)":round(audio_max*100,2),
#                                 "Classification":classification})
# 
#             except Exception as e:
#                 st.warning(f"Failed {cand['file']}: {e}")
# 
#         df=pd.DataFrame(results)
#         df.to_csv(REPORT_FILE,index=False)
#         st.success(f"‚úÖ Verification complete! {len(df)} videos analyzed.")
#         st.download_button("üì• Download Report CSV", df.to_csv(index=False), file_name=REPORT_FILE, mime="text/csv")
#         st.write("### Candidate Video Previews")
#         for row in results:
#             st.markdown(f"**{row['Classification']} ‚Äî @{row['Username']} ‚Äî CLIP {row['CLIP Sim (%)']}%**")
#

from pyngrok import ngrok

# Kill all existing tunnels first
ngrok.kill()

import subprocess
from pyngrok import ngrok

PORT = 8503  # Streamlit port

# Start Streamlit in background
subprocess.Popen(["streamlit", "run", "video_app.py", "--server.port", str(PORT)])

# Expose via ngrok
NGROK_TOKEN = "33c6zLu0TJnlczZmzaMZF8GbwlA_5d1kftwFyEEHnZM8Z3wJa"
ngrok.set_auth_token(NGROK_TOKEN)
public_url = ngrok.connect(PORT)

print(f"üöÄ Streamlit app is live at: {public_url}")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile video_app.py
# import os
# import json
# import hashlib
# import subprocess
# import tempfile
# from pathlib import Path
# from typing import List, Dict, Any
# 
# import streamlit as st
# import pandas as pd
# import numpy as np
# from PIL import Image
# import imagehash
# import requests
# import librosa
# import torch
# from transformers import CLIPProcessor, CLIPModel
# import tweepy
# 
# # ---------------------------
# # Config / folders
# # ---------------------------
# OFFICIAL_DIR = "official_videos"
# CANDIDATE_DIR = "candidate_videos"
# FRAME_TMP_DIR = "tmp_frames"
# LEDGER_FILE = "ledger.json"
# REPORT_FILE = "video_detection_report.csv"
# 
# os.makedirs(OFFICIAL_DIR, exist_ok=True)
# os.makedirs(CANDIDATE_DIR, exist_ok=True)
# os.makedirs(FRAME_TMP_DIR, exist_ok=True)
# 
# VIS_THRESH = 0.85
# PHASH_HAMMING_THRESH = 10
# PHASH_MATCH_RATE = 0.6
# AUDIO_SIM_THRESH = 0.8
# 
# # ---------------------------
# # Utilities
# # ---------------------------
# def compute_sha256(path: str) -> str:
#     h = hashlib.sha256()
#     with open(path, "rb") as f:
#         for chunk in iter(lambda: f.read(8192), b""):
#             h.update(chunk)
#     return h.hexdigest()
# 
# def run_ffmpeg_extract_frames(video_path: str, out_dir: str, fps: float = 1.0) -> List[str]:
#     os.makedirs(out_dir, exist_ok=True)
#     out_pattern = os.path.join(out_dir, "frame_%06d.jpg")
#     cmd = ["ffmpeg","-y","-i",video_path,"-vf",f"fps={fps}","-q:v","2",out_pattern]
#     subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
#     return sorted([str(p) for p in Path(out_dir).glob("frame_*.jpg")])
# 
# def compute_phash_for_image(image_path: str) -> str:
#     img = Image.open(image_path).convert("RGB")
#     return str(imagehash.phash(img))
# 
# def extract_audio_to_wav(video_path: str, wav_path: str, sr: int = 22050, max_duration: float = 60.0):
#     cmd = ["ffmpeg","-y","-i",video_path,"-vn","-ac","1","-ar",str(sr),"-t",str(max_duration),wav_path]
#     subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
# 
# def compute_audio_mfcc_mean(video_path: str) -> List[float]:
#     with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
#         wav_path = tmp_wav.name
#     try:
#         extract_audio_to_wav(video_path, wav_path)
#         y, _ = librosa.load(wav_path, sr=22050, mono=True, duration=60.0)
#         if len(y)==0:
#             return []
#         mfcc = librosa.feature.mfcc(y=y, sr=22050, n_mfcc=20)
#         return np.mean(mfcc, axis=1).tolist()
#     finally:
#         try: os.remove(wav_path)
#         except: pass
# 
# def phash_hamming_distance(hex1: str, hex2: str) -> int:
#     try: return (int(hex1,16) ^ int(hex2,16)).bit_count()
#     except: return 999
# 
# def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
#     if a is None or b is None: return 0.0
#     denom = np.linalg.norm(a)*np.linalg.norm(b)
#     if denom==0: return 0.0
#     return float(np.dot(a,b)/denom)
# 
# # ---------------------------
# # CLIP Model
# # ---------------------------
# @st.cache_resource(show_spinner=False)
# def load_clip_model(device: str = None):
#     if device is None:
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#     model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
#     processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
#     model.to(device)
#     model.eval()
#     return model, processor, device
# 
# def compute_clip_frame_embedding(image_path: str, model, processor, device) -> np.ndarray:
#     img = Image.open(image_path).convert("RGB")
#     inputs = processor(images=img, return_tensors="pt")
#     inputs = {k:v.to(device) for k,v in inputs.items()}
#     with torch.no_grad():
#         outputs = model.get_image_features(**inputs)
#     vec = outputs[0].cpu().numpy()
#     norm = np.linalg.norm(vec)
#     if norm>0: vec = vec/norm
#     return vec.flatten()
# 
# def avg_pool_frame_embeddings(frame_paths: List[str], model, processor, device) -> np.ndarray:
#     vecs=[]
#     for p in frame_paths:
#         try: vecs.append(compute_clip_frame_embedding(p, model, processor, device))
#         except: continue
#     if not vecs: return np.zeros((512,),dtype=np.float32)
#     mean = np.mean(np.vstack(vecs), axis=0)
#     norm = np.linalg.norm(mean)
#     if norm>0: mean=mean/norm
#     return mean
# 
# # ---------------------------
# # Ledger functions
# # ---------------------------
# def load_ledger(path: str = LEDGER_FILE) -> Dict[str, Any]:
#     if os.path.exists(path):
#         try:
#             with open(path,"r") as f: return json.load(f)
#         except: return {}
#     return {}
# 
# def save_ledger(ledger: Dict[str, Any], path: str = LEDGER_FILE):
#     with open(path,"w") as f: json.dump(ledger,f,indent=2)
# 
# # ---------------------------
# # Streamlit UI
# # ---------------------------
# st.set_page_config(page_title="Video Verification Dashboard", layout="wide")
# st.title("üé• Video Verification ‚Äî Movie / Trailer Authenticity")
# 
# # Sidebar
# st.sidebar.header("Settings")
# sampled_fps = st.sidebar.slider("Sample frames per second (fps)",0.5,2.0,1.0,0.5)
# vis_thresh = st.sidebar.slider("Visual similarity threshold",0.6,0.99,VIS_THRESH,0.01)
# phash_hamm = st.sidebar.slider("pHash Hamming distance threshold",0,64,PHASH_HAMMING_THRESH,1)
# phash_rate_thresh = st.sidebar.slider("pHash match rate threshold",0.1,1.0,PHASH_MATCH_RATE,0.05)
# audio_sim_thresh = st.sidebar.slider("Audio similarity threshold",0.1,1.0,AUDIO_SIM_THRESH,0.01)
# 
# # ---------------------------
# # Upload Official Videos
# # ---------------------------
# st.subheader("1Ô∏è‚É£ Upload Official Videos")
# uploaded_offs = st.file_uploader("Upload official videos", accept_multiple_files=True, type=["mp4","mov","mkv","webm"])
# if uploaded_offs:
#     if "official_paths" not in st.session_state: st.session_state.official_paths=[]
#     for uf in uploaded_offs:
#         save_path=os.path.join(OFFICIAL_DIR,uf.name)
#         if save_path not in st.session_state.official_paths:
#             with open(save_path,"wb") as f: f.write(uf.getbuffer())
#             st.session_state.official_paths.append(save_path)
#     st.success(f"Saved {len(st.session_state.official_paths)} official videos.")
# 
# # ---------------------------
# # Build / Update Ledger
# # ---------------------------
# st.subheader("2Ô∏è‚É£ Build / Update Ledger")
# build_btn = st.button("üîß Build/Update Ledger")
# if build_btn:
#     if not hasattr(st.session_state,"official_paths") or not st.session_state.official_paths:
#         st.error("No official videos uploaded.")
#     else:
#         with st.spinner("Loading CLIP model..."):
#             model, processor, device = load_clip_model()
#         ledger = load_ledger()
#         for i,p in enumerate(st.session_state.official_paths,1):
#             st.info(f"Processing: {os.path.basename(p)} ({i}/{len(st.session_state.official_paths)})")
#             tmpdir = tempfile.mkdtemp(prefix="off_frames_")
#             frames = run_ffmpeg_extract_frames(p,tmpdir,fps=sampled_fps)
#             phashes=[compute_phash_for_image(f) for f in frames]
#             emb=avg_pool_frame_embeddings(frames,model,processor,device)
#             audio_mfcc=compute_audio_mfcc_mean(p)
#             ledger[os.path.basename(p)]={
#                 "path":p,
#                 "sha256":compute_sha256(p),
#                 "phashes":phashes,
#                 "clip_emb":emb.tolist(),
#                 "audio_mfcc_mean":audio_mfcc,
#                 "sampled_fps":sampled_fps
#             }
#             for f in Path(tmpdir).glob("*"): f.unlink()
#             try: Path(tmpdir).rmdir()
#             except: pass
#         save_ledger(ledger)
#         st.success("Ledger build/update complete!")
# 
# # ---------------------------
# # Fetch candidate videos from X API
# # ---------------------------
# st.subheader("3Ô∏è‚É£ Fetch candidate videos from X")
# bearer_token = st.text_input("Enter X API Bearer Token (required for fetching candidate videos)")
# movie_query = st.text_input("Movie / Trailer search query")
# fetch_btn = st.button("‚ñ∂Ô∏è Fetch candidate videos")
# 
# if fetch_btn:
#     if not bearer_token or not movie_query:
#         st.error("Provide both X API token and search query")
#     else:
#         client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)
#         query = f"{movie_query} has:videos -is:retweet"
#         try:
#             tweets = client.search_recent_tweets(
#                 query=query,
#                 max_results=30,
#                 expansions=["attachments.media_keys","author_id"],
#                 media_fields=["url","type","duration_ms"],
#                 tweet_fields=["id","author_id","created_at","text"]
#             )
#         except Exception as e:
#             st.error(f"Error fetching tweets: {e}")
#             tweets=None
# 
#         if tweets and tweets.data:
#             authors = {u.id:u.username for u in tweets.includes.get("users",[])} if tweets.includes else {}
#             media_map = {m.media_key:m for m in tweets.includes.get("media",[])} if tweets.includes else {}
#             downloaded=[]
#             for t in tweets.data:
#                 if "attachments" in t.data:
#                     for mk in t.data["attachments"]["media_keys"]:
#                         if mk in media_map and media_map[mk].type=="video":
#                             url = media_map[mk].url
#                             if url is None:
#                                 continue
#                             save_path=os.path.join(CANDIDATE_DIR,f"{t.id}.mp4")
#                             try:
#                                 resp = requests.get(url)
#                                 with open(save_path,"wb") as f: f.write(resp.content)
#                                 downloaded.append({"tweet_id":t.id,"username":authors.get(t.author_id,"unknown"),
#                                                    "created_at":str(t.created_at),"text":t.text[:100],"file":save_path})
#                             except:
#                                 continue
#             st.session_state.candidates=downloaded
#             st.success(f"Downloaded {len(downloaded)} candidate videos")
#         else:
#             st.warning("No candidate videos found")
# 
# # ---------------------------
# # Verification
# # ---------------------------
# verify_btn = st.button("‚úÖ Run Verification")
# if verify_btn:
#     if "candidates" not in st.session_state or not st.session_state.candidates:
#         st.error("No candidate videos to verify")
#     else:
#         st.info("Running verification...")
#         with st.spinner("Loading CLIP model..."):
#             model, processor, device = load_clip_model()
#         ledger = load_ledger()
#         results=[]
#         for cand in st.session_state.candidates:
#             try:
#                 frames=run_ffmpeg_extract_frames(cand["file"],FRAME_TMP_DIR,fps=sampled_fps)
#                 cand_phashes=[compute_phash_for_image(f) for f in frames]
#                 phash_matches=[]
#                 for off_data in ledger.values():
#                     total=len(off_data["phashes"])
#                     match_count=sum(phash_hamming_distance(p1,p2)<=phash_hamm for p1,p2 in zip(cand_phashes,off_data["phashes"]))
#                     phash_matches.append(match_count/total if total>0 else 0.0)
#                 phash_max=max(phash_matches) if phash_matches else 0.0
# 
#                 cand_emb=avg_pool_frame_embeddings(frames,model,processor,device)
#                 clip_sims=[cosine_sim(cand_emb,np.array(off_data["clip_emb"])) for off_data in ledger.values()]
#                 clip_max=max(clip_sims) if clip_sims else 0.0
# 
#                 cand_audio=compute_audio_mfcc_mean(cand["file"])
#                 audio_sims=[cosine_sim(np.array(off_data["audio_mfcc_mean"]),np.array(cand_audio)) if off_data["audio_mfcc_mean"] and cand_audio else 0.0 for off_data in ledger.values()]
#                 audio_max=max(audio_sims) if audio_sims else 0.0
# 
#                 classification="Irrelevant ‚ö†Ô∏è"
#                 if clip_max>vis_thresh:
#                     if phash_max>phash_rate_thresh:
#                         cand_sha=compute_sha256(cand["file"])
#                         matched_off_name=list(ledger.keys())[np.argmax(clip_sims)]
#                         if cand_sha==ledger[matched_off_name]["sha256"]:
#                             classification="Official ‚úÖ"
#                         else:
#                             classification="Fake/Doctored ‚ùå"
#                     else:
#                         classification="Repost üîÅ"
# 
#                 results.append({"Tweet ID":cand["tweet_id"],
#                                 "Tweet URL":f"https://x.com/{cand['username']}/status/{cand['tweet_id']}",
#                                 "Username":cand["username"],
#                                 "Posted At":cand["created_at"],
#                                 "Tweet Snippet":cand["text"],
#                                 "pHash Match":round(phash_max*100,2),
#                                 "CLIP Sim (%)":round(clip_max*100,2),
#                                 "Audio Sim (%)":round(audio_max*100,2),
#                                 "Classification":classification})
#             except Exception as e:
#                 st.warning(f"Failed {cand['file']}: {e}")
# 
#         df=pd.DataFrame(results)
#         df.to_csv(REPORT_FILE,index=False)
#         st.success(f"‚úÖ Verification complete! {len(df)} videos analyzed.")
#         st.download_button("üì• Download Report CSV", df.to_csv(index=False), file_name=REPORT_FILE, mime="text/csv")
#         st.write("### Candidate Video Previews")
#         for row in results:
#             st.markdown(f"**{row['Classification']} ‚Äî @{row['Username']} ‚Äî CLIP {row['CLIP Sim (%)']}%**")
#

# ------------------------------
# 1Ô∏è‚É£ Install dependencies
# ------------------------------
!pip install --quiet streamlit pyngrok tweepy transformers torch torchvision torchaudio pillow librosa imagehash

# ------------------------------
# 2Ô∏è‚É£ Save Streamlit app
# ------------------------------
streamlit_code = """
import os
import json
import hashlib
import subprocess
import tempfile
from pathlib import Path
from typing import List, Dict, Any

import streamlit as st
import pandas as pd
import numpy as np
from PIL import Image
import imagehash
import requests
import librosa
import torch
from transformers import CLIPProcessor, CLIPModel
import tweepy

# --------------------------- Config / folders
OFFICIAL_DIR = "official_videos"
CANDIDATE_DIR = "candidate_videos"
FRAME_TMP_DIR = "tmp_frames"
LEDGER_FILE = "ledger.json"
REPORT_FILE = "video_detection_report.csv"

os.makedirs(OFFICIAL_DIR, exist_ok=True)
os.makedirs(CANDIDATE_DIR, exist_ok=True)
os.makedirs(FRAME_TMP_DIR, exist_ok=True)

VIS_THRESH = 0.85
PHASH_HAMMING_THRESH = 10
PHASH_MATCH_RATE = 0.6
AUDIO_SIM_THRESH = 0.8

# --------------------------- Utilities
def compute_sha256(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()

def run_ffmpeg_extract_frames(video_path: str, out_dir: str, fps: float = 1.0) -> List[str]:
    os.makedirs(out_dir, exist_ok=True)
    out_pattern = os.path.join(out_dir, "frame_%06d.jpg")
    cmd = ["ffmpeg","-y","-i",video_path,"-vf",f"fps={fps}","-q:v","2",out_pattern]
    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
    return sorted([str(p) for p in Path(out_dir).glob("frame_*.jpg")])

def compute_phash_for_image(image_path: str) -> str:
    img = Image.open(image_path).convert("RGB")
    return str(imagehash.phash(img))

def extract_audio_to_wav(video_path: str, wav_path: str, sr: int = 22050, max_duration: float = 60.0):
    cmd = ["ffmpeg","-y","-i",video_path,"-vn","-ac","1","-ar",str(sr),"-t",str(max_duration),wav_path]
    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)

def compute_audio_mfcc_mean(video_path: str) -> List[float]:
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
        wav_path = tmp_wav.name
    try:
        extract_audio_to_wav(video_path, wav_path)
        y, _ = librosa.load(wav_path, sr=22050, mono=True, duration=60.0)
        if len(y)==0:
            return []
        mfcc = librosa.feature.mfcc(y=y, sr=22050, n_mfcc=20)
        return np.mean(mfcc, axis=1).tolist()
    finally:
        try: os.remove(wav_path)
        except: pass

def phash_hamming_distance(hex1: str, hex2: str) -> int:
    try: return (int(hex1,16) ^ int(hex2,16)).bit_count()
    except: return 999

def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
    if a is None or b is None: return 0.0
    denom = np.linalg.norm(a)*np.linalg.norm(b)
    if denom==0: return 0.0
    return float(np.dot(a,b)/denom)

# --------------------------- CLIP Model
@st.cache_resource(show_spinner=False)
def load_clip_model(device: str = None):
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    model.to(device)
    model.eval()
    return model, processor, device

def compute_clip_frame_embedding(image_path: str, model, processor, device) -> np.ndarray:
    img = Image.open(image_path).convert("RGB")
    inputs = processor(images=img, return_tensors="pt")
    inputs = {k:v.to(device) for k,v in inputs.items()}
    with torch.no_grad():
        outputs = model.get_image_features(**inputs)
    vec = outputs[0].cpu().numpy()
    norm = np.linalg.norm(vec)
    if norm>0: vec = vec/norm
    return vec.flatten()

def avg_pool_frame_embeddings(frame_paths: List[str], model, processor, device) -> np.ndarray:
    vecs=[]
    for p in frame_paths:
        try: vecs.append(compute_clip_frame_embedding(p, model, processor, device))
        except: continue
    if not vecs: return np.zeros((512,),dtype=np.float32)
    mean = np.mean(np.vstack(vecs), axis=0)
    norm = np.linalg.norm(mean)
    if norm>0: mean=mean/norm
    return mean

# --------------------------- Ledger functions
def load_ledger(path: str = LEDGER_FILE) -> Dict[str, Any]:
    if os.path.exists(path):
        try:
            with open(path,"r") as f: return json.load(f)
        except: return {}
    return {}

def save_ledger(ledger: Dict[str, Any], path: str = LEDGER_FILE):
    with open(path,"w") as f: json.dump(ledger,f,indent=2)

# --------------------------- Streamlit UI
st.set_page_config(page_title="Video Verification", layout="wide")
st.title("üé• Video Verification ‚Äî Movie / Trailer Authenticity")

# Sidebar settings
st.sidebar.header("Settings")
sampled_fps = st.sidebar.slider("Sample frames per second (fps)",0.5,2.0,1.0,0.5)
vis_thresh = st.sidebar.slider("Visual similarity threshold",0.6,0.99,VIS_THRESH,0.01)
phash_hamm = st.sidebar.slider("pHash Hamming distance threshold",0,64,PHASH_HAMMING_THRESH,1)
phash_rate_thresh = st.sidebar.slider("pHash match rate threshold",0.1,1.0,PHASH_MATCH_RATE,0.05)
audio_sim_thresh = st.sidebar.slider("Audio similarity threshold",0.1,1.0,AUDIO_SIM_THRESH,0.01)

# --------------------------- 1Ô∏è‚É£ Upload Official Videos
st.subheader("1Ô∏è‚É£ Upload Official Videos")
uploaded_offs = st.file_uploader("Upload official videos", accept_multiple_files=True, type=["mp4","mov","mkv","webm"])
if uploaded_offs:
    if "official_paths" not in st.session_state: st.session_state.official_paths=[]
    for uf in uploaded_offs:
        save_path=os.path.join(OFFICIAL_DIR,uf.name)
        if save_path not in st.session_state.official_paths:
            with open(save_path,"wb") as f: f.write(uf.getbuffer())
            st.session_state.official_paths.append(save_path)
    st.success(f"Saved {len(st.session_state.official_paths)} official videos.")

# --------------------------- 2Ô∏è‚É£ Build / Update Ledger
st.subheader("2Ô∏è‚É£ Build / Update Ledger")
build_btn = st.button("üîß Build/Update Ledger")
if build_btn:
    if not hasattr(st.session_state,"official_paths") or not st.session_state.official_paths:
        st.error("No official videos uploaded.")
    else:
        with st.spinner("Loading CLIP model..."):
            model, processor, device = load_clip_model()
        ledger = load_ledger()
        for i,p in enumerate(st.session_state.official_paths,1):
            st.info(f"Processing: {os.path.basename(p)} ({i}/{len(st.session_state.official_paths)})")
            tmpdir = tempfile.mkdtemp(prefix="off_frames_")
            frames = run_ffmpeg_extract_frames(p,tmpdir,fps=sampled_fps)
            phashes=[compute_phash_for_image(f) for f in frames]
            emb=avg_pool_frame_embeddings(frames,model,processor,device)
            audio_mfcc=compute_audio_mfcc_mean(p)
            ledger[os.path.basename(p)]={
                "path":p,
                "sha256":compute_sha256(p),
                "phashes":phashes,
                "clip_emb":emb.tolist(),
                "audio_mfcc_mean":audio_mfcc,
                "sampled_fps":sampled_fps
            }
            for f in Path(tmpdir).glob("*"): f.unlink()
            try: Path(tmpdir).rmdir()
            except: pass
        save_ledger(ledger)
        st.success("Ledger build/update complete!")

# --------------------------- 3Ô∏è‚É£ Fetch candidate videos from X
st.subheader("3Ô∏è‚É£ Fetch candidate videos from X API")
bearer_token = st.text_input("Enter X API Bearer Token")
movie_query = st.text_input("Movie / Trailer search query")
fetch_btn = st.button("‚ñ∂Ô∏è Fetch candidate videos")

if fetch_btn:
    if not bearer_token or not movie_query:
        st.error("Provide both X API token and search query")
    else:
        try:
            client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)
            query = f"{movie_query} has:videos -is:retweet"
            tweets = client.search_recent_tweets(
                query=query,
                max_results=30,
                expansions=["attachments.media_keys","author_id"],
                media_fields=["url","type","duration_ms"],
                tweet_fields=["id","author_id","created_at","text"]
            )
        except Exception as e:
            st.error(f"Error fetching tweets: {e}")
            tweets = None

        if tweets and tweets.data:
            authors = {u.id:u.username for u in tweets.includes.get("users",[])} if tweets.includes else {}
            media_map = {m.media_key:m for m in tweets.includes.get("media",[])} if tweets.includes else {}
            downloaded=[]
            for t in tweets.data:
                if "attachments" in t.data:
                    for mk in t.data["attachments"]["media_keys"]:
                        if mk in media_map and media_map[mk].type=="video":
                            url = media_map[mk].url
                            if url is None:
                                continue
                            save_path=os.path.join(CANDIDATE_DIR,f"{t.id}.mp4")
                            try:
                                resp = requests.get(url)
                                with open(save_path,"wb") as f: f.write(resp.content)
                                downloaded.append({"tweet_id":t.id,"username":authors.get(t.author_id,"unknown"),
                                                   "created_at":str(t.created_at),"text":t.text[:100],"file":save_path})
                            except:
                                continue
            st.session_state.candidates = downloaded
            st.success(f"Downloaded {len(downloaded)} candidate videos")
        else:
            st.warning("No candidate videos found")

# --------------------------- 4Ô∏è‚É£ Verification
verify_btn = st.button("‚úÖ Run Verification")
if verify_btn:
    if "candidates" not in st.session_state or not st.session_state.candidates:
        st.error("No candidate videos to verify")
    else:
        st.info("Running verification...")
        with st.spinner("Loading CLIP model..."):
            model, processor, device = load_clip_model()
        ledger = load_ledger()
        results=[]
        for cand in st.session_state.candidates:
            try:
                frames=run_ffmpeg_extract_frames(cand["file"],FRAME_TMP_DIR,fps=sampled_fps)
                cand_phashes=[compute_phash_for_image(f) for f in frames]
                phash_matches=[]
                for off_data in ledger.values():
                    total=len(off_data["phashes"])
                    match_count=sum(phash_hamming_distance(p1,p2)<=phash_hamm for p1,p2 in zip(cand_phashes,off_data["phashes"]))
                    phash_matches.append(match_count/total if total>0 else 0.0)
                phash_max=max(phash_matches) if phash_matches else 0.0

                cand_emb=avg_pool_frame_embeddings(frames,model,processor,device)
                clip_sims=[cosine_sim(cand_emb,np.array(off_data["clip_emb"])) for off_data in ledger.values()]
                clip_max=max(clip_sims) if clip_sims else 0.0

                cand_audio=compute_audio_mfcc_mean(cand["file"])
                audio_sims=[cosine_sim(np.array(off_data["audio_mfcc_mean"]),np.array(cand_audio)) if off_data["audio_mfcc_mean"] and cand_audio else 0.0 for off_data in ledger.values()]
                audio_max=max(audio_sims) if audio_sims else 0.0

                classification="Irrelevant ‚ö†Ô∏è"
                if clip_max>vis_thresh:
                    if phash_max>phash_rate_thresh:
                        cand_sha=compute_sha256(cand["file"])
                        matched_off_name=list(ledger.keys())[np.argmax(clip_sims)]
                        if cand_sha==ledger[matched_off_name]["sha256"]:
                            classification="Official ‚úÖ"
                        else:
                            classification="Fake/Doctored ‚ùå"
                    else:
                        classification="Repost üîÅ"

                results.append({"Tweet ID":cand["tweet_id"],
                                "Tweet URL":f"https://x.com/{cand['username']}/status/{cand['tweet_id']}",
                                "Username":cand["username"],
                                "Posted At":cand["created_at"],
                                "Tweet Snippet":cand["text"],
                                "pHash Match":round(phash_max*100,2),
                                "CLIP Sim (%)":round(clip_max*100,2),
                                "Audio Sim (%)":round(audio_max*100,2),
                                "Classification":classification})
            except Exception as e:
                st.warning(f"Failed {cand['file']}: {e}")

        df=pd.DataFrame(results)
        df.to_csv(REPORT_FILE,index=False)
        st.success(f"‚úÖ Verification complete! {len(df)} videos analyzed.")
        st.download_button("üì• Download Report CSV", df.to_csv(index=False), file_name=REPORT_FILE, mime="text/csv")
        st.write("### Candidate Video Previews")
        for row in results:
            st.markdown(f"**{row['Classification']} ‚Äî @{row['Username']} ‚Äî CLIP {row['CLIP Sim (%)']}%**")
"""

with open("video_app.py", "w") as f:
    f.write(streamlit_code)

# ------------------------------
# 3Ô∏è‚É£ Setup ngrok
# ------------------------------
from pyngrok import ngrok
import os, time

PORT = 8506
NGROK_TOKEN = "33c6zLu0TJnlczZmzaMZF8GbwlA_5d1kftwFyEEHnZM8Z3wJa"

ngrok.kill()
ngrok.set_auth_token(NGROK_TOKEN)

# ------------------------------
# 4Ô∏è‚É£ Run Streamlit in background
# ------------------------------
os.system(f"nohup streamlit run video_app.py --server.port {PORT} > streamlit.log 2>&1 &")
time.sleep(5)

# ------------------------------
# 5Ô∏è‚É£ Start ngrok tunnel
# ------------------------------
public_url = ngrok.connect(PORT)
print(f"üöÄ Streamlit app is live at: {public_url}")

