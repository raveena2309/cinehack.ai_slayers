{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "736d61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90e50437",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILES_DIR = \"D:\\B.Tech\\Hackathons\\CinehackAI\\Harmful Data\\POC DATA\\Text\"  # Update this path\n",
    "OUTPUT_DIR = \"models\"\n",
    "BERT_MODEL = \"bert-base-uncased\"\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 128\n",
    "RANDOM_SEED = 42\n",
    "SAFE_SPEECH_RATIO = 1.0  # 1:1 ratio with harmful speech\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82035649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. LOAD AND PARSE JSON FILES\n",
    "# ============================================\n",
    "def inspect_json_structure(file_path, max_depth=3):\n",
    "    \"\"\"Detailed inspection of JSON file structure\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return\n",
    "    \n",
    "    def inspect_obj(obj, depth=0, path=\"root\"):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "        \n",
    "        indent = \"  \" * depth\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            print(f\"{indent}{path} [dict] with {len(obj)} keys\")\n",
    "            for key, value in list(obj.items())[:5]:  # Show first 5 keys\n",
    "                print(f\"{indent}  - '{key}': {type(value).__name__}\", end=\"\")\n",
    "                if isinstance(value, str):\n",
    "                    preview = value[:50].replace('\\n', ' ')\n",
    "                    print(f\" = '{preview}...'\")\n",
    "                elif isinstance(value, (list, dict)):\n",
    "                    print(f\" (len={len(value)})\")\n",
    "                else:\n",
    "                    print()\n",
    "                \n",
    "                if depth < max_depth - 1:\n",
    "                    inspect_obj(value, depth + 1, f\"{path}.{key}\")\n",
    "        \n",
    "        elif isinstance(obj, list):\n",
    "            print(f\"{indent}{path} [list] with {len(obj)} items\")\n",
    "            if len(obj) > 0:\n",
    "                print(f\"{indent}  First item type: {type(obj[0]).__name__}\")\n",
    "                if depth < max_depth - 1:\n",
    "                    inspect_obj(obj[0], depth + 1, f\"{path}[0]\")\n",
    "    \n",
    "    inspect_obj(data)\n",
    "\n",
    "def extract_text_from_json(obj, texts=None):\n",
    "    \"\"\"Recursively extract all text fields from JSON\"\"\"\n",
    "    if texts is None:\n",
    "        texts = []\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        # Common text field names - INCLUDING img_text for your dataset!\n",
    "        for key in ['img_text', 'text', 'content', 'message', 'comment', 'post', 'tweet', \n",
    "                    'description', 'body', 'title', 'caption', 'review', 'label_text']:\n",
    "            if key in obj:\n",
    "                value = obj[key]\n",
    "                if isinstance(value, str) and len(value.strip()) > 0:\n",
    "                    texts.append(value.strip())\n",
    "        \n",
    "        # Recursively search nested dicts\n",
    "        for value in obj.values():\n",
    "            if isinstance(value, (dict, list)):\n",
    "                extract_text_from_json(value, texts)\n",
    "    \n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            if isinstance(item, (dict, list)):\n",
    "                extract_text_from_json(item, texts)\n",
    "    \n",
    "    return texts\n",
    "\n",
    "def load_json_files(directory):\n",
    "    \"\"\"Load all JSON files from directory\"\"\"\n",
    "    json_files = list(Path(directory).glob(\"*.json\"))\n",
    "    all_texts = []\n",
    "    failed_files = []\n",
    "    \n",
    "    if len(json_files) == 0:\n",
    "        raise ValueError(f\"No JSON files found in {directory}\")\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    # Detailed inspection of first 3 files\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INSPECTING FIRST 3 FILES FOR STRUCTURE\")\n",
    "    print(\"=\"*60)\n",
    "    for i, file_path in enumerate(json_files[:3]):\n",
    "        print(f\"\\n[File {i+1}] {file_path.name}\")\n",
    "        print(\"-\" * 60)\n",
    "        inspect_json_structure(file_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    user_input = input(\"Press ENTER to continue loading all files, or type 'stop' to exit: \")\n",
    "    if user_input.lower() == 'stop':\n",
    "        raise KeyboardInterrupt(\"User stopped execution\")\n",
    "    \n",
    "    print(\"\\nLoading all files...\")\n",
    "    for file_path in tqdm(json_files, desc=\"Loading JSON files\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                extracted = extract_text_from_json(data)\n",
    "                all_texts.extend(extracted)\n",
    "        except Exception as e:\n",
    "            failed_files.append((file_path.name, str(e)))\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"\\nWarning: Failed to load {len(failed_files)} files\")\n",
    "        if len(failed_files) <= 10:\n",
    "            for fname, error in failed_files:\n",
    "                print(f\"  - {fname}: {error}\")\n",
    "        else:\n",
    "            print(\"First 10 failures:\")\n",
    "            for fname, error in failed_files[:10]:\n",
    "                print(f\"  - {fname}: {error}\")\n",
    "    \n",
    "    return all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c665321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. TEXT PREPROCESSING\n",
    "# ============================================\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove mentions and hashtags (but keep the text after them)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d69de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# 3. SYNTHETIC SAFE SPEECH GENERATION\n",
    "# ============================================\n",
    "def generate_safe_speech(harmful_texts, ratio=1.0):\n",
    "    \"\"\"Generate synthetic safe speech data\"\"\"\n",
    "    \n",
    "    # Templates for safe speech\n",
    "    safe_templates = [\n",
    "        # Positive statements\n",
    "        \"I really enjoyed {topic} today, it was wonderful\",\n",
    "        \"Thank you for sharing your thoughts on {topic}\",\n",
    "        \"I appreciate the discussion about {topic}\",\n",
    "        \"Learning about {topic} has been very helpful\",\n",
    "        \"The {topic} session was informative and well-organized\",\n",
    "        \n",
    "        # Neutral statements\n",
    "        \"I'm interested in learning more about {topic}\",\n",
    "        \"Can you provide information on {topic}?\",\n",
    "        \"What's your opinion on {topic}?\",\n",
    "        \"I've been thinking about {topic} lately\",\n",
    "        \"The article about {topic} was interesting\",\n",
    "        \n",
    "        # Encouraging statements\n",
    "        \"Great work on the {topic} project\",\n",
    "        \"I'm happy to help with {topic}\",\n",
    "        \"Let's collaborate on {topic}\",\n",
    "        \"Your perspective on {topic} is valuable\",\n",
    "        \"I respect your view on {topic}\",\n",
    "        \n",
    "        # General conversation\n",
    "        \"Have a great day everyone\",\n",
    "        \"Looking forward to our next meeting\",\n",
    "        \"Thanks for your time and consideration\",\n",
    "        \"I hope you're doing well\",\n",
    "        \"Best wishes for your project\",\n",
    "        \"That's a good point to consider\",\n",
    "        \"I understand what you mean\",\n",
    "        \"Let me think about that\",\n",
    "        \"That makes sense to me\",\n",
    "        \"I see your perspective\"\n",
    "    ]\n",
    "    \n",
    "    topics = [\n",
    "        \"technology\", \"education\", \"sports\", \"music\", \"art\", \"science\",\n",
    "        \"literature\", \"food\", \"travel\", \"health\", \"fitness\", \"nature\",\n",
    "        \"history\", \"culture\", \"business\", \"environment\", \"community\",\n",
    "        \"family\", \"friends\", \"hobbies\", \"movies\", \"books\", \"photography\",\n",
    "        \"gaming\", \"cooking\", \"design\", \"architecture\", \"astronomy\"\n",
    "    ]\n",
    "    \n",
    "    num_safe = int(len(harmful_texts) * ratio)\n",
    "    safe_texts = []\n",
    "    \n",
    "    print(f\"Generating {num_safe} synthetic safe speech examples...\")\n",
    "    \n",
    "    for _ in range(num_safe):\n",
    "        template = np.random.choice(safe_templates)\n",
    "        if '{topic}' in template:\n",
    "            topic = np.random.choice(topics)\n",
    "            text = template.format(topic=topic)\n",
    "        else:\n",
    "            text = template\n",
    "        \n",
    "        # Add variation\n",
    "        if np.random.random() > 0.5:\n",
    "            text = text.capitalize()\n",
    "        if np.random.random() > 0.7:\n",
    "            text += \".\"\n",
    "        \n",
    "        safe_texts.append(text)\n",
    "    \n",
    "    return safe_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8595fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. BERT ENCODING\n",
    "# ============================================\n",
    "class BERTEncoder:\n",
    "    def __init__(self, model_name=BERT_MODEL):\n",
    "        print(f\"Loading BERT model: {model_name}\")\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "            self.model = BertModel.from_pretrained(model_name).to(self.device)\n",
    "            self.model.eval()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load BERT model: {e}\")\n",
    "    \n",
    "    def encode_batch(self, texts, max_length=MAX_LENGTH):\n",
    "        \"\"\"Encode a batch of texts\"\"\"\n",
    "        try:\n",
    "            encoded = self.tokenizer.batch_encode_plus(\n",
    "                texts,\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = encoded['input_ids'].to(self.device)\n",
    "            attention_mask = encoded['attention_mask'].to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                # Use [CLS] token embedding\n",
    "                embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            \n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error encoding batch: {e}\")\n",
    "    \n",
    "    def encode_texts(self, texts, batch_size=BATCH_SIZE):\n",
    "        \"\"\"Encode all texts in batches\"\"\"\n",
    "        if len(texts) == 0:\n",
    "            raise ValueError(\"No texts to encode! Check your JSON files.\")\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        try:\n",
    "            for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding texts\"):\n",
    "                batch = texts[i:i + batch_size]\n",
    "                embeddings = self.encode_batch(batch)\n",
    "                all_embeddings.append(embeddings)\n",
    "            \n",
    "            return np.vstack(all_embeddings)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during text encoding: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "befec59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5. MAIN PIPELINE\n",
    "# ============================================\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BERT + XGBoost Hate Speech Classification Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load harmful texts\n",
    "        print(\"\\n[1/7] Loading harmful texts from JSON files...\")\n",
    "        harmful_texts_raw = load_json_files(JSON_FILES_DIR)\n",
    "        print(f\"Loaded {len(harmful_texts_raw)} harmful texts\")\n",
    "        \n",
    "        if len(harmful_texts_raw) == 0:\n",
    "            raise ValueError(\"No texts were extracted from JSON files! Check your JSON structure.\")\n",
    "        \n",
    "        # Step 2: Preprocess harmful texts\n",
    "        print(\"\\n[2/7] Preprocessing harmful texts...\")\n",
    "        harmful_texts = [preprocess_text(text) for text in harmful_texts_raw]\n",
    "        harmful_texts = [text for text in harmful_texts if len(text) > 10]  # Filter short texts\n",
    "        print(f\"After preprocessing: {len(harmful_texts)} harmful texts\")\n",
    "        \n",
    "        if len(harmful_texts) == 0:\n",
    "            raise ValueError(\"All texts were filtered out during preprocessing!\")\n",
    "        \n",
    "        # Step 3: Generate safe texts\n",
    "        print(\"\\n[3/7] Generating synthetic safe speech...\")\n",
    "        safe_texts = generate_safe_speech(harmful_texts, ratio=SAFE_SPEECH_RATIO)\n",
    "        \n",
    "        # Create dataset\n",
    "        all_texts = harmful_texts + safe_texts\n",
    "        all_labels = [1] * len(harmful_texts) + [0] * len(safe_texts)  # 1=harmful, 0=safe\n",
    "        \n",
    "        print(f\"\\nDataset summary:\")\n",
    "        print(f\"  Harmful texts: {len(harmful_texts)}\")\n",
    "        print(f\"  Safe texts: {len(safe_texts)}\")\n",
    "        print(f\"  Total: {len(all_texts)}\")\n",
    "        \n",
    "        # Step 4: Encode with BERT\n",
    "        print(\"\\n[4/7] Encoding texts with BERT...\")\n",
    "        encoder = BERTEncoder()\n",
    "        embeddings = encoder.encode_texts(all_texts)\n",
    "        print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        \n",
    "        # Step 5: Train-test split\n",
    "        print(\"\\n[5/7] Splitting dataset...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            embeddings, all_labels, test_size=0.2, random_state=RANDOM_SEED, stratify=all_labels\n",
    "        )\n",
    "        print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "        print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "        \n",
    "        # Step 6: Train XGBoost with efficient approach\n",
    "        print(\"\\n[6/7] Training XGBoost classifier...\")\n",
    "        \n",
    "        # Split train into train/validation for monitoring\n",
    "        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=RANDOM_SEED, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        print(\"Training XGBoost with optimized parameters...\")\n",
    "        \n",
    "        # Check XGBoost version for compatibility\n",
    "        xgb_version = tuple(map(int, xgb.__version__.split('.')[:2]))\n",
    "        \n",
    "        if xgb_version >= (2, 0):\n",
    "            # XGBoost 2.0+ syntax\n",
    "            model = xgb.XGBClassifier(\n",
    "                objective='binary:logistic',\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                n_estimators=300,\n",
    "                min_child_weight=3,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                gamma=0.1,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=RANDOM_SEED,\n",
    "                tree_method='hist',\n",
    "                n_jobs=-1,\n",
    "                early_stopping_rounds=50\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_train_split,\n",
    "                y_train_split,\n",
    "                eval_set=[(X_val_split, y_val_split)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            # XGBoost 1.x syntax\n",
    "            model = xgb.XGBClassifier(\n",
    "                objective='binary:logistic',\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                n_estimators=300,\n",
    "                min_child_weight=3,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                gamma=0.1,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=RANDOM_SEED,\n",
    "                use_label_encoder=False,\n",
    "                tree_method='hist',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            model.fit(\n",
    "                X_train_split,\n",
    "                y_train_split,\n",
    "                eval_set=[(X_val_split, y_val_split)],\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=False\n",
    "            )\n",
    "        \n",
    "        # Validation score\n",
    "        y_val_pred = model.predict(X_val_split)\n",
    "        val_f1 = f1_score(y_val_split, y_val_pred)\n",
    "        print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "        \n",
    "        # Get best iteration\n",
    "        best_n_estimators = getattr(model, 'best_iteration', None)\n",
    "        if best_n_estimators is None:\n",
    "            best_n_estimators = 300\n",
    "        else:\n",
    "            best_n_estimators = int(best_n_estimators)\n",
    "        print(f\"Best iteration: {best_n_estimators}\")\n",
    "        \n",
    "        # Retrain on full training set\n",
    "        print(\"Training final model on full training set...\")\n",
    "        if xgb_version >= (2, 0):\n",
    "            best_model = xgb.XGBClassifier(\n",
    "                objective='binary:logistic',\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                n_estimators=best_n_estimators,\n",
    "                min_child_weight=3,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                gamma=0.1,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=RANDOM_SEED,\n",
    "                tree_method='hist',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        else:\n",
    "            best_model = xgb.XGBClassifier(\n",
    "                objective='binary:logistic',\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                n_estimators=best_n_estimators,\n",
    "                min_child_weight=3,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                gamma=0.1,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=RANDOM_SEED,\n",
    "                use_label_encoder=False,\n",
    "                tree_method='hist',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        \n",
    "        best_model.fit(X_train, y_train, verbose=False)\n",
    "        \n",
    "        # Step 7: Evaluate\n",
    "        print(\"\\n[7/7] Evaluating model...\")\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"FINAL RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred, \n",
    "                                    target_names=['Safe', 'Harmful'],\n",
    "                                    digits=4))\n",
    "        \n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"                Predicted\")\n",
    "        print(f\"                Safe  Harmful\")\n",
    "        print(f\"Actual Safe     {cm[0][0]:4d}  {cm[0][1]:4d}\")\n",
    "        print(f\"       Harmful  {cm[1][0]:4d}  {cm[1][1]:4d}\")\n",
    "        \n",
    "        # Save model and encoder\n",
    "        print(f\"\\n[Saving] Saving model to {OUTPUT_DIR}/\")\n",
    "        with open(f\"{OUTPUT_DIR}/xgboost_model.pkl\", 'wb') as f:\n",
    "            pickle.dump(best_model, f)\n",
    "        \n",
    "        # Save preprocessing info\n",
    "        model_info = {\n",
    "            'accuracy': float(accuracy),\n",
    "            'f1_score': float(f1),\n",
    "            'num_harmful': len(harmful_texts),\n",
    "            'num_safe': len(safe_texts),\n",
    "            'xgboost_version': xgb.__version__,\n",
    "            'bert_model': BERT_MODEL\n",
    "        }\n",
    "        \n",
    "        with open(f\"{OUTPUT_DIR}/model_info.json\", 'w') as f:\n",
    "            json.dump(model_info, f, indent=2)\n",
    "        \n",
    "        print(\"\\n✓ Model saved successfully!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return best_model, encoder\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f485a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6. INFERENCE FUNCTION\n",
    "# ============================================\n",
    "def predict_text(text, model, encoder):\n",
    "    \"\"\"Predict if a text is harmful or safe\"\"\"\n",
    "    try:\n",
    "        processed = preprocess_text(text)\n",
    "        if len(processed) == 0:\n",
    "            return {\n",
    "                'text': text,\n",
    "                'prediction': 'UNKNOWN',\n",
    "                'harmful_probability': 0.0,\n",
    "                'safe_probability': 0.0,\n",
    "                'error': 'Text too short after preprocessing'\n",
    "            }\n",
    "        \n",
    "        embedding = encoder.encode_texts([processed])\n",
    "        prediction = model.predict(embedding)[0]\n",
    "        probability = model.predict_proba(embedding)[0]\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'prediction': 'HARMFUL' if prediction == 1 else 'SAFE',\n",
    "            'harmful_probability': float(probability[1]),\n",
    "            'safe_probability': float(probability[0])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'text': text,\n",
    "            'prediction': 'ERROR',\n",
    "            'harmful_probability': 0.0,\n",
    "            'safe_probability': 0.0,\n",
    "            'error': str(e)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b287d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BERT + XGBoost Hate Speech Classification Pipeline\n",
      "============================================================\n",
      "\n",
      "[1/7] Loading harmful texts from JSON files...\n",
      "Found 701 JSON files\n",
      "\n",
      "============================================================\n",
      "INSPECTING FIRST 3 FILES FOR STRUCTURE\n",
      "============================================================\n",
      "\n",
      "[File 1] 1023940826882293760.json\n",
      "------------------------------------------------------------\n",
      "root [dict] with 1 keys\n",
      "  - 'img_text': str = 'İ'M SLOWLY BEC«MİNG RETARpEp! ...'\n",
      "\n",
      "[File 2] 1023940897346658307.json\n",
      "------------------------------------------------------------\n",
      "root [dict] with 1 keys\n",
      "  - 'img_text': str = '* 36% 10, ull Verizon LTE 10:37 AM Tweet nicktendo...'\n",
      "\n",
      "[File 3] 1023943177319919616.json\n",
      "------------------------------------------------------------\n",
      "root [dict] with 1 keys\n",
      "  - 'img_text': str = 'Silverwing Hold Antact Power 0n 1C 36 10 Alliance ...'\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading all files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading JSON files: 100%|██████████| 701/701 [00:00<00:00, 2445.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 701 harmful texts\n",
      "\n",
      "[2/7] Preprocessing harmful texts...\n",
      "After preprocessing: 576 harmful texts\n",
      "\n",
      "[3/7] Generating synthetic safe speech...\n",
      "Generating 576 synthetic safe speech examples...\n",
      "\n",
      "Dataset summary:\n",
      "  Harmful texts: 576\n",
      "  Safe texts: 576\n",
      "  Total: 1152\n",
      "\n",
      "[4/7] Encoding texts with BERT...\n",
      "Loading BERT model: bert-base-uncased\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts: 100%|██████████| 36/36 [00:20<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1152, 768)\n",
      "\n",
      "[5/7] Splitting dataset...\n",
      "Train set: 921 samples\n",
      "Test set: 231 samples\n",
      "\n",
      "[6/7] Training XGBoost classifier...\n",
      "Training XGBoost with optimized parameters...\n",
      "Validation F1 Score: 1.0000\n",
      "Best iteration: 291\n",
      "Training final model on full training set...\n",
      "\n",
      "[7/7] Evaluating model...\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "\n",
      "Accuracy: 0.9827\n",
      "F1 Score: 0.9823\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Safe     0.9667    1.0000    0.9831       116\n",
      "     Harmful     1.0000    0.9652    0.9823       115\n",
      "\n",
      "    accuracy                         0.9827       231\n",
      "   macro avg     0.9833    0.9826    0.9827       231\n",
      "weighted avg     0.9833    0.9827    0.9827       231\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted\n",
      "                Safe  Harmful\n",
      "Actual Safe      116     0\n",
      "       Harmful     4   111\n",
      "\n",
      "[Saving] Saving model to models/\n",
      "\n",
      "✓ Model saved successfully!\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "SAMPLE PREDICTIONS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts: 100%|██████████| 1/1 [00:00<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: I hate all people from that country\n",
      "Prediction: HARMFUL\n",
      "Harmful probability: 0.9575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts: 100%|██████████| 1/1 [00:00<00:00, 48.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: Thank you for your helpful feedback\n",
      "Prediction: SAFE\n",
      "Harmful probability: 0.1071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts: 100%|██████████| 1/1 [00:00<00:00, 49.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: You're so stupid and worthless\n",
      "Prediction: HARMFUL\n",
      "Harmful probability: 0.9920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts: 100%|██████████| 1/1 [00:00<00:00, 35.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: Have a wonderful day everyone!\n",
      "Prediction: SAFE\n",
      "Harmful probability: 0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        model, encoder = main()\n",
    "        \n",
    "        # Test predictions\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SAMPLE PREDICTIONS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        test_samples = [\n",
    "            \"I hate all people from that country\",\n",
    "            \"Thank you for your helpful feedback\",\n",
    "            \"You're so stupid and worthless\",\n",
    "            \"Have a wonderful day everyone!\"\n",
    "        ]\n",
    "        \n",
    "        for sample in test_samples:\n",
    "            result = predict_text(sample, model, encoder)\n",
    "            print(f\"\\nText: {result['text']}\")\n",
    "            print(f\"Prediction: {result['prediction']}\")\n",
    "            if 'error' not in result:\n",
    "                print(f\"Harmful probability: {result['harmful_probability']:.4f}\")\n",
    "            else:\n",
    "                print(f\"Error: {result['error']}\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExecution stopped by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nFatal error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
