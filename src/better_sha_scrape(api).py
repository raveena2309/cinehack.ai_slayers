# -*- coding: utf-8 -*-
"""better-sha_scrape(api).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rF8_jMdxWpVAY_ba8a99mQzJqCZYOf2N

entire streamlit
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import os, requests, pandas as pd
# from io import BytesIO
# from PIL import Image
# import tweepy
# from transformers import pipeline
# import time
# 
# # ================================
# # Setup folders
# # ================================
# os.makedirs("candidate_images", exist_ok=True)
# 
# # ================================
# # Streamlit UI
# # ================================
# st.title("üö® Harmful Image Detection Dashboard")
# st.markdown("""
# Analyze up to **20 recent image posts** on **X (Twitter)** to detect
# **harmful, hateful, or NSFW content** using a pretrained AI model.
# """)
# 
# bearer_token = st.text_input("Enter your X (Twitter) API Bearer Token", type="password")
# search_query = st.text_input("Enter search query (e.g. 'violence meme', 'hate speech poster')")
# run_detection = st.button("üöÄ Run Detection")
# 
# # ================================
# # Detection Logic
# # ================================
# if run_detection:
#     if not bearer_token or not search_query:
#         st.error("Please provide both Bearer Token and a search query.")
#     else:
#         st.info("Fetching and analyzing tweets... Please wait ‚è≥")
# 
#         # Initialize Twitter client
#         try:
#             client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)
#         except Exception as e:
#             st.error(f"Error initializing X API client: {e}")
#             st.stop()
# 
#         # Query X API (limit 20 images)
#         query = f"{search_query} has:images -is:retweet"
#         try:
#             tweets = client.search_recent_tweets(
#                 query=query,
#                 max_results=20,
#                 tweet_fields=["id","created_at","author_id","text"],
#                 expansions=["attachments.media_keys","author_id"],
#                 media_fields=["url"]
#             )
#         except Exception as e:
#             st.error(f"Error fetching tweets: {e}")
#             st.stop()
# 
#         author_map = {user.id:user.username for user in tweets.includes.get("users", [])} if tweets.includes else {}
#         media_map = {m.media_key:m for m in tweets.includes.get("media", [])} if tweets.includes else {}
# 
#         candidates = []
#         if tweets.data:
#             for tweet in tweets.data:
#                 if "attachments" in tweet.data:
#                     for mkey in tweet.data["attachments"]["media_keys"]:
#                         if len(candidates) >= 20:
#                             break  # stop after 20 images total
#                         if mkey in media_map:
#                             url = media_map[mkey].url
#                             try:
#                                 response = requests.get(url, timeout=10)
#                                 img = Image.open(BytesIO(response.content)).convert("RGB")
#                                 save_path = f"candidate_images/{tweet.id}.jpg"
#                                 img.save(save_path)
#                                 candidates.append({
#                                     "tweet_id": tweet.id,
#                                     "username": author_map.get(tweet.author_id, "unknown"),
#                                     "created_at": str(tweet.created_at),
#                                     "text": tweet.text[:120],
#                                     "file": save_path,
#                                     "url": f"https://x.com/{author_map.get(tweet.author_id, 'unknown')}/status/{tweet.id}"
#                                 })
#                             except Exception as e:
#                                 print("Error downloading image:", e)
#                 if len(candidates) >= 20:
#                     break
#         else:
#             st.warning("No recent image tweets found for this query.")
#             st.stop()
# 
#         st.success(f"‚úÖ Downloaded {len(candidates)} images for analysis.")
# 
#         # ================================
#         # Harmful Image Classifier
#         # ================================
#         st.info("Analyzing images with pretrained AI model...")
#         classifier = pipeline("image-classification", model="Falconsai/nsfw_image_detection")
# 
#         results = []
#         for i, item in enumerate(candidates):
#             try:
#                 preds = classifier(Image.open(item["file"]))
#                 label = preds[0]["label"]
#                 score = round(preds[0]["score"] * 100, 2)
#                 results.append({
#                     "Tweet URL": item["url"],
#                     "Username": item["username"],
#                     "Posted At": item["created_at"],
#                     "Tweet Text": item["text"],
#                     "Predicted Label": label,
#                     "Confidence (%)": score
#                 })
#                 time.sleep(1)  # rate-limit safety delay
#             except Exception as e:
#                 print(f"Error analyzing {item['file']}: {e}")
# 
#         # ================================
#         # Display Results
#         # ================================
#         df = pd.DataFrame(results)
#         df.to_csv("harmful_image_report.csv", index=False)
#         st.success("Analysis complete! ‚úÖ")
#         st.download_button(
#             label="üì• Download Report as CSV",
#             data=df.to_csv(index=False),
#             file_name="harmful_image_report.csv",
#             mime="text/csv"
#         )
# 
#         st.write("### üñºÔ∏è Detection Results")
#         for i, row in df.iterrows():
#             st.markdown(f"**{row['Predicted Label']} ({row['Confidence (%)']}%) ‚Äî @{row['Username']}**")
#             st.image(f"candidate_images/{candidates[i]['tweet_id']}.jpg", use_column_width=True)
#

!pip install streamlit pyngrok tweepy transformers pillow pandas torch --quiet

from pyngrok import ngrok
import os, time

# ‚úÖ Kill old ngrok tunnels to avoid endpoint limit
ngrok.kill()

PORT = 8501
APP_FILE = "app.py"

# Start Streamlit in background
get_ipython().system_raw(f"streamlit run {APP_FILE} --server.port {PORT} --server.headless true &")

# Wait a bit for app to start
time.sleep(10)

# Open a new tunnel safely
public_url = ngrok.connect(PORT)
print(f"üöÄ Streamlit app is live at: {public_url}")

